{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-14T02:41:33.714561Z",
     "start_time": "2024-12-14T02:41:32.118004Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[HAMI-core Msg(51615:139915000765312:libvgpu.c:836)]: Initializing.....\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T02:59:35.291409Z",
     "start_time": "2024-12-05T02:59:35.284135Z"
    }
   },
   "cell_type": "code",
   "source": "torch.__version__",
   "id": "a25661e88748016",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0+cu121'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 基本数据处理与计算操作\n",
    "#### 创建Tensor"
   ],
   "id": "8e7335dd4f25c2a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T03:15:35.403637Z",
     "start_time": "2024-12-05T03:15:35.381789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 创建一个2x3的未初始化的Tensor\n",
    "x1 = torch.empty(2, 3)\n",
    "# 创建一个2x3的随机初始化的Tensor\n",
    "x2 = torch.rand(2,3)\n",
    "# 创建一个2x3的long型全0的Tensor\n",
    "x3 = torch.zeros(2, 3, dtype=torch.long)\n",
    "# 为每一个元素以给定的mean和std用高斯分布生成随机数\n",
    "x4 = torch.normal(mean=0.5, std=torch.arange(1., 6.))\n",
    "# randn 服从N(0，1)的正态分布\n",
    "x5 = torch.randn(2, 3)\n",
    "\n",
    "x1, x2, x3, x4, x5"
   ],
   "id": "f98c4fe2f9122de7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0825e-34, 0.0000e+00, 0.0000e+00],\n",
       "         [1.4013e-45, 0.0000e+00, 0.0000e+00]]),\n",
       " tensor([[0.9814, 0.0874, 0.5718],\n",
       "         [0.6293, 0.1877, 0.5226]]),\n",
       " tensor([[0, 0, 0],\n",
       "         [0, 0, 0]]),\n",
       " tensor([ 1.5466, -3.7387, -2.4466,  3.9320,  0.3221]),\n",
       " tensor([[-1.7507, -0.7708, -0.1977],\n",
       "         [ 1.7947,  2.4549, -0.1717]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T05:50:40.808737Z",
     "start_time": "2024-12-05T05:50:40.797425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 根据不同数据类型创建\n",
    "x1 = torch.tensor([[5,5,3], [2,2,5]])\n",
    "x2 = torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "x3 = torch.from_numpy(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "x1, x2, x3"
   ],
   "id": "b88af3712e08564",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5, 5, 3],\n",
       "         [2, 2, 5]]),\n",
       " tensor([[1, 2, 3],\n",
       "         [4, 5, 6]]),\n",
       " tensor([[1, 2, 3],\n",
       "         [4, 5, 6]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T05:52:26.204311Z",
     "start_time": "2024-12-05T05:52:26.185551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([[1,2,3], [4,5,6]])\n",
    "# 返回的tensor默认具有相同的torch.dtype和torch.device \n",
    "x1 = x.new_ones(2, 3, dtype=torch.float64)\n",
    "# 指定新的数据类型\n",
    "x2 = torch.randn_like(x, dtype=torch.float, device='cuda')\n",
    "x1, x2"
   ],
   "id": "efb85eb5dc01eb70",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.]], dtype=torch.float64),\n",
       " tensor([[ 1.2797, -0.0880,  1.2151],\n",
       "         [ 1.1600, -0.2267,  0.4583]], device='cuda:0'))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 操作张量",
   "id": "e0b51474809fc04c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### 算术操作",
   "id": "404c8168a71d8e79"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T06:04:43.481923Z",
     "start_time": "2024-12-05T06:04:43.468071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.ones(2, 3) \n",
    "y = torch.eye(2, 3)\n",
    "# 加法形式1\n",
    "s1 = x + y\n",
    "# 加法形式2\n",
    "s2 = torch.add(x, y)\n",
    "# 加法形式3，inplace(原地操作)，原值修改\n",
    "s3 = y.add_(x)\n",
    "\n",
    "x, y, s1, s2, s3, y"
   ],
   "id": "c38d174958713b62",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[2., 1., 1.],\n",
       "         [1., 2., 1.]]),\n",
       " tensor([[2., 1., 1.],\n",
       "         [1., 2., 1.]]),\n",
       " tensor([[2., 1., 1.],\n",
       "         [1., 2., 1.]]),\n",
       " tensor([[2., 1., 1.],\n",
       "         [1., 2., 1.]]),\n",
       " tensor([[2., 1., 1.],\n",
       "         [1., 2., 1.]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### 索引\n",
    "索引出来的结果与原数据`共享内存`，也即修改一个，另一个会跟着修改。"
   ],
   "id": "e9e8ea08923b768c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T06:10:21.218064Z",
     "start_time": "2024-12-05T06:10:21.203136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.ones(2, 3, device='cuda') \n",
    "y = x[0, :]\n",
    "y += 1\n",
    "\n",
    "x, y, x[0, :]"
   ],
   "id": "952e713dea06fdb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 2., 2.],\n",
       "         [1., 1., 1.]], device='cuda:0'),\n",
       " tensor([2., 2., 2.], device='cuda:0'),\n",
       " tensor([2., 2., 2.], device='cuda:0'))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### 切片 index_select\n",
    "沿着指定维度对输入进行切片，取index中指定的相应项(index 为一个 LongTensor)，然后返回到一个新的张量， 返回的张量与原始张量 Tensor 有相同的维度(在指定轴上)。\n",
    "\n",
    "注意： `返回的张量不与原始张量共享内存空间`。<br>\n",
    "\n",
    "参数:<br>\n",
    "input (Tensor)         – 输入张量<br>\n",
    "dim (int)              – 索引的轴<br>\n",
    "index (LongTensor)     – 包含索引下标的一维张量<br>\n",
    "out (Tensor, optional) – 目标张量<br>"
   ],
   "id": "4c83e29022cd4abd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T06:28:21.135491Z",
     "start_time": "2024-12-05T06:28:21.119048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.randn(3, 4)\n",
    "indices = torch.LongTensor([0, 2])\n",
    "y = torch.index_select(x, 0, indices)\n",
    "x, indices, y"
   ],
   "id": "be273576e54333f9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1912,  0.1199, -0.0856, -1.8215],\n",
       "         [ 0.2932,  0.9630,  0.3461, -0.7105],\n",
       "         [-1.2408, -1.6029,  0.8402,  0.3389]]),\n",
       " tensor([0, 2]),\n",
       " tensor([[ 0.1912,  0.1199, -0.0856, -1.8215],\n",
       "         [-1.2408, -1.6029,  0.8402,  0.3389]]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### 改变形状 view/reshape\n",
    "1. `view()`<br>\n",
    "注意view()**返回的新Tensor与源Tensor虽然可能有不同的size**，但`共享data`。<br>\n",
    "即更改其中的一个，另外一个也会跟着改变。(顾名思义，view仅仅是改变了对这个张量的 观察角度，内部数据并未改变)\n",
    "2. `reshape()` 和 `clone()`<br> \n",
    "Pytorch还提供了一个 reshape() 方法可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。<br> \n",
    "我们推荐先用 clone() 创造一个副本然后再使用view()。"
   ],
   "id": "638e71766e4e379d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T06:37:26.690979Z",
     "start_time": "2024-12-05T06:37:26.674120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.ones(2, 4)\n",
    "# 一个tensor必须是连续的contiguous()才能被查看。\n",
    "# 一开始不加contiguous()，报 “view size is not compatible ... ” 错误\n",
    "y1 = x.view(8)\n",
    "y2 = x.view(-1, 8) # -1所指的维度可以根据其他维度的值推出来\n",
    "\n",
    "x, y1, y2, x.size(), y1.size(), y2.size()"
   ],
   "id": "228a589372df84e0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " tensor([[1., 1., 1., 1., 1., 1., 1., 1.]]),\n",
       " torch.Size([2, 4]),\n",
       " torch.Size([8]),\n",
       " torch.Size([1, 8]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T06:37:27.557078Z",
     "start_time": "2024-12-05T06:37:27.548856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x += 1\n",
    "print(x)\n",
    "print(y1) # 也加了1"
   ],
   "id": "ff5073877939dce0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.]])\n",
      "tensor([2., 2., 2., 2., 2., 2., 2., 2.])\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T06:40:50.680881Z",
     "start_time": "2024-12-05T06:40:50.665059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# torch.view()会改变原始张量，但是很多情况下，我们希望原始张量和变换后的张量互相不影响。为为了使创建的张量和原始张量不共享内存，我们需要使用第二种方法torch.reshape()， 同样可以改变张量的形状，但是此函数并不能保证返回的是其拷贝值，所以官方不推荐使用。推荐的方法是我们先用 `clone()` 创造一个张量副本然后再使用 `torch.view()`进行函数维度变换。<br>\n",
    "# 注：使用 clone() 还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源 Tensor 。 \n",
    "\n",
    "x1 = x.clone()\n",
    "y3 = x.view(-1)\n",
    "x1 += 1\n",
    "\n",
    "x1, y3"
   ],
   "id": "2966e0478d013f3d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3.]]),\n",
       " tensor([2., 2., 2., 2., 2., 2., 2., 2.]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### gather\n",
    "`torch.gather(input, dim, index, out=None)`<br>\n",
    "官方定义：沿给定轴dim，将输入索引张量index指定位置的值进行聚合。<br>\n",
    "通俗理解：给定轴dim，在input中，根据index指定的下标，选择元素重组成一个新的tensor，最后输出的out与index的size是一样的。<br>\n",
    "\n",
    "对一个3维张量，输出可以定义为<br>\n",
    "\n",
    "```\n",
    "out[i][j][k] = tensor[index[i][j][k]][j][k]] # dim=0\n",
    "out[i][j][k] = tensor[i][index[i][j][k]][k]] # dim=1\n",
    "out[i][j][k] = tensor[i][j][index[i][j][k]]] # dim=3\n",
    "```"
   ],
   "id": "4c4092d912b18775"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T06:44:34.623357Z",
     "start_time": "2024-12-05T06:44:34.607203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_input = torch.tensor([[0.9, 0, 0], [0, 0.6, 0],[0, 0.7, 0]])\n",
    "index = torch.tensor([0, 1, 1])\n",
    "_input, index, index.view(-1, 1)"
   ],
   "id": "c461832c1b04d76f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.6000, 0.0000],\n",
       "         [0.0000, 0.7000, 0.0000]]),\n",
       " tensor([0, 1, 1]),\n",
       " tensor([[0],\n",
       "         [1],\n",
       "         [1]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.9000],\n        [0.6000],\n        [0.7000]])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30,
   "source": [
    "_input.gather(dim=1, index=index.view(-1, 1))\n",
    "# dim=1，表示的是在第二维度上操作。\n",
    "# 在index中，[0]表示第一行对应元素的下标，即[0.9]\n",
    "#           [1]表示第二行对应元素的下标，即[0.6]\n",
    "#           [1]表示第三行对应元素的下标，即[0.7]\n"
   ],
   "id": "2dd910f8efdeb599"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 广播机制\n",
    "当我们对两个形状不同的Tensor按元素运算时，可能会触发`广播(broadcasting)机制`。 <br>\n",
    "先适当复制元素使这两个Tensor形状相同后再按元素运算。"
   ],
   "id": "70802a520b875fca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T06:51:29.369464Z",
     "start_time": "2024-12-05T06:51:29.354588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# x 中第一行的2个 元素被广播(复制)到了第二行和第三行\n",
    "# y 中第一列的3个元素被广播(复制)到 了第二列\n",
    "x = torch.arange(1, 3).view(1, 2)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "x, y, x+y"
   ],
   "id": "b84ba14b9b1fd304",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2]]),\n",
       " tensor([[1],\n",
       "         [2],\n",
       "         [3]]),\n",
       " tensor([[2, 3],\n",
       "         [3, 4],\n",
       "         [4, 5]]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tensor和NumPy相互转换\n",
    "+ `numpy()`和`from_numpy()` <br>\n",
    "这两个函数所产生的的Tensor和NumPy中的数组共享相同的内存(所以他们之间的转换很快)，改变其中一个时另一个也会改变!\n",
    "+ `torch.tensor()` <br>\n",
    "进行**数据拷贝**，所以返回的Tensor和原来 的数据不再共享内存。"
   ],
   "id": "a074be98b2d79f77"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T06:57:00.723692Z",
     "start_time": "2024-12-05T06:57:00.708576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tensor转NumPy数组\n",
    "x = torch.ones(3)\n",
    "y = x.numpy()\n",
    "x += 1\n",
    "y += 1\n",
    "print(x, y)\n",
    "\n",
    "# NumPy数组转 Tensor\n",
    "x = np.ones(3)\n",
    "y = torch.from_numpy(x)\n",
    "x += 1\n",
    "y += 1\n",
    "print(x, y)"
   ],
   "id": "43545e1b02a013db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3.]) [3. 3. 3.]\n",
      "[3. 3. 3.] tensor([3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T06:58:12.393425Z",
     "start_time": "2024-12-05T06:58:12.380853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 使用torch.tensor()将NumPy数组转换成Tensor(不再共享内存)\n",
    "x = np.ones(3)\n",
    "y = torch.tensor(x)\n",
    "x += 1\n",
    "print(x, y)"
   ],
   "id": "d70a731be351d2d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2.] tensor([1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### (8) 降维\n",
    "在 PyTorch 中，对张量进行聚合操作时（例如求和或求均值），默认情况下，会调用相关函数沿指定的轴进行聚合，并降低张量的维度。例如：<br>\n",
    "指定 axis=0：沿轴 0 汇总所有行的元素降维（轴0），因此，输入轴0的维数在输出形状中消失。<br>\n",
    "指定 axis=1：沿轴 1 汇总所有列的元素降维（轴1），因此，输入轴1的维数在输出形状中消失。<br>\n",
    "全矩阵求和：同时对行和列进行求和，相当于对矩阵的所有元素求和，最终得到一个标量。\n"
   ],
   "id": "b6898cae957d9385"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T09:41:38.135889Z",
     "start_time": "2024-12-13T09:41:38.120931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "A = torch.arange(6, dtype=torch.float32).view(3, 2)\n",
    "print(A)\n",
    "print(A.shape, A.sum(), A.sum(axis=[0, 1]))\n",
    "\n",
    "A_sum_axis0 = A.sum(axis=0)\n",
    "print(A_sum_axis0.shape, A_sum_axis0)\n",
    "\n",
    "A_sum_axis1 = A.sum(axis=1)\n",
    "print(A_sum_axis1.shape, A_sum_axis1)"
   ],
   "id": "d0247f41d99be33f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1.],\n",
      "        [2., 3.],\n",
      "        [4., 5.]])\n",
      "torch.Size([3, 2]) tensor(15.) tensor(15.)\n",
      "torch.Size([2]) tensor([6., 9.])\n",
      "torch.Size([3]) tensor([1., 5., 9.])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "如果希望保持轴数不变，可以使用 `keepdim=True` 参数。这样就可以保持张量的维度一致，方便后续操作，特别是在进行广播（broadcasting）或者在特定形状的计算中非常有用。",
   "id": "b3bf5002162286c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T09:56:34.606021Z",
     "start_time": "2024-12-13T09:56:34.591845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "A = torch.arange(6, dtype=torch.float32).view(2, 3)\n",
    "B = torch.arange(6, dtype=torch.float32).view(2, 3)\n",
    "\n",
    "A_sum_axis0 = A.sum(axis=0)\n",
    "print(A)\n",
    "print(A_sum_axis0)\n",
    "print(B)\n",
    "print(A_sum_axis0 + B)"
   ],
   "id": "f60b5b3f6ca19250",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n",
      "tensor([3., 5., 7.])\n",
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n",
      "tensor([[ 3.,  6.,  9.],\n",
      "        [ 6.,  9., 12.]])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### (9) 点积和矩阵-向量积\n",
    "**点积**（dot product）<br>\n",
    "给定两个向量 $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^d$ ， \n",
    "它们的*点积*是 $\\mathbf{x}^\\top\\mathbf{y}$ = $\\sum_{i=1}^{d} x_i y_i$ 。\n",
    "即两个向量相同位置的按元素乘积的和。\n",
    "<br>\n",
    "<br>\n",
    "**矩阵-向量积**（matrix-vector product）<br>\n",
    "给定矩阵 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ 和向量 $\\mathbf{x} \\in \\mathbb{R}^n$ 。<br>\n",
    "矩阵 $\\mathbf{A}$ 用它的行向量表示：\n",
    "\n",
    "$$\\mathbf{A}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix},$$\n",
    "\n",
    "其中每个 $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^n$ 都是行向量，表示矩阵的第 $i$ 行。\n",
    "矩阵向量积 $\\mathbf{A}\\mathbf{x}$ 是一个长度为 $m$ 的列向量，其第 $i$ 个元素是点积 $\\mathbf{a}^\\top_i \\mathbf{x}$ ：\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    " \\mathbf{a}^\\top_{1} \\mathbf{x}  \\\\\n",
    " \\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n",
    "\\vdots\\\\\n",
    " \\mathbf{a}^\\top_{m} \\mathbf{x}\\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "**矩阵‐矩阵乘法**（matrix‐matrix multiplication）<br>\n",
    "假设有两个矩阵 $\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$ 和 $\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$， $\\mathbf{A}$ 用行向量表， $\\mathbf{B}$ 用列向量表示，\n",
    "则矩阵积 $\\mathbf{C} = \\mathbf{A}\\mathbf{B}$ 的每个元素 $c_{ij}$ 可以计算为点积 $\\mathbf{a}^\\top_i \\mathbf{b}_j$:\n",
    "\n",
    "$$\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\\n",
    " \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\\n",
    " \\vdots & \\vdots & \\ddots &\\vdots\\\\\n",
    "\\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m\n",
    "\\end{bmatrix}.\n",
    "$$"
   ],
   "id": "a7c5bb8042a8ce80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T15:12:41.892132Z",
     "start_time": "2024-12-13T15:12:41.885619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.arange(4, dtype=torch.float32)\n",
    "y = torch.ones(4, dtype = torch.float32)\n",
    "_dot = torch.sum(x * y)\n",
    "x, y, torch.dot(x, y), _dot"
   ],
   "id": "35f290c20b28f79c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.), tensor(6.))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T15:17:02.062419Z",
     "start_time": "2024-12-13T15:17:02.056216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "A = torch.arange(8, dtype=torch.float32).view(2, 4)\n",
    "x = torch.arange(4, dtype=torch.float32)\n",
    "print(A)\n",
    "print(x)\n",
    "A.shape, x.shape, torch.mv(A, x)"
   ],
   "id": "23c606e8a15709d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2., 3.],\n",
      "        [4., 5., 6., 7.]])\n",
      "tensor([0., 1., 2., 3.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4]), torch.Size([4]), tensor([14., 38.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T15:34:40.482267Z",
     "start_time": "2024-12-13T15:34:40.476073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "A = torch.arange(8, dtype=torch.float32).view(2, 4)\n",
    "B = torch.arange(12, dtype=torch.float32).view(4, 3)\n",
    "C = torch.mm(A, B)\n",
    "A.shape, B.shape, C.shape"
   ],
   "id": "c0a6e3a5c1fa8f86",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4]), torch.Size([4, 3]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### (10) 范数\n",
    "向量的**范数**（norm）将向量映射到标量，表示一个向量有多大。假设 $n$ 维向量 $\\mathbf{x}$ 中的元素是 $x_1,\\ldots,x_n$ ，那么：<br>\n",
    "\n",
    "**$L_1$ 范数**：$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|$ ，表示向量元素的绝对值之和。<br>\n",
    "**$L_2$ 范数**：$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}$ ，表示向量元素平方和的平方根。<br>\n",
    "**$L_p$ 范数**：$\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}$ <br>\n",
    "\n",
    "类似于向量的$L_2$范数，**矩阵** $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$ 的**Frobenius范数**（Frobenius norm）：<br>\n",
    "\n",
    "$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$ ，表示矩阵元素平方和的平方根。\n"
   ],
   "id": "bb70ab0e30f5f844"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T02:46:12.137683Z",
     "start_time": "2024-12-14T02:46:12.115322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([3.0, -4.0])\n",
    "l1_1 = torch.abs(x).sum()\n",
    "l1_2 = torch.norm(x, p=1)\n",
    "l2_1 = torch.norm(x)\n",
    "l2_2 = torch.norm(x, p=2)\n",
    "\n",
    "X = torch.ones(3, 4)\n",
    "f_1 = torch.norm(X)\n",
    "f_2 = torch.norm(X, p='fro')\n",
    "print(l1_1, l1_2, l2_1, l2_2)\n",
    "print(f_1, f_2)"
   ],
   "id": "c502f054befcca5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.) tensor(7.) tensor(5.) tensor(5.)\n",
      "tensor(3.4641) tensor(3.4641)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 自动求梯度（Autograd）\n",
    "torch.Tensor 是这个包的核心类。如果设置它的属性 `.requires_grad` 为 True，那么它将会追踪对于该张量的所有操作。\n",
    "当完成计算后可以通过调用 `.backward()`，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到`.grad`属性。<br>\n",
    "\n",
    "如果不想要被继续追踪，可以调用`.detach()`将其从追踪记录中分离出来，这样就可以防止将来的计算被追踪。\n",
    "此外，还可以用`with torch.no_grad()`将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用，因为在评估模型时，我们并不需要计算 可训练参数(requires_grad=True)的梯度。<br>\n",
    "\n",
    "Tensor和Function互相结合就可以构建一个记录有整个计算过程的**有向无环图(DAG)**。\n",
    "每个Tensor都有一个`.grad_fn`属性，用来记录创建张量时所用到的运算，在链式求导法则中会使用到，默认是None。<br>\n",
    "\n",
    "+ 自动求导机制通过有向无环图（directed acyclic graph ，DAG）实现\n",
    "+ 在DAG中，记录数据（对应tensor.data）以及操作（对应tensor.grad_fn）\n",
    "+ 操作在pytorch中统称为`Function`，如加法、减法、乘法、ReLU、conv、Pooling等"
   ],
   "id": "24e03c74202568ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T12:06:21.296623Z",
     "start_time": "2024-12-05T12:06:21.285971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 标量求梯度\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "a = torch.add(w, x)\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    " # x是直接创建的，所以它没有grad_fn\n",
    "print(x.grad_fn)\n",
    "print(a.grad_fn)\n",
    "\n",
    "y.backward()\n",
    "print(w.grad)"
   ],
   "id": "b0554114476ba53b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<AddBackward0 object at 0x7f14f9b04c10>\n",
      "tensor([5.])\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T12:06:23.778396Z",
     "start_time": "2024-12-05T12:06:23.767866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 注意grad是累加的\n",
    "y2 = w.sum()\n",
    "y2.backward()      # 梯度未清零，累加梯度\n",
    "print(w.grad)\n",
    "\n",
    "y3 = w.sum()\n",
    "w.grad.data.zero_()\n",
    "y3.backward()      # 梯度清零后，x的梯度为1\n",
    "print(w.grad)"
   ],
   "id": "7d8696f460361684",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.])\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T12:15:49.144354Z",
     "start_time": "2024-12-05T12:15:49.124765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "class Exp(Function):                    # 此层计算e^x\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):                # 模型前向\n",
    "        result = i.exp()\n",
    "        ctx.save_for_backward(result)   # 保存所需内容，以备backward时使用，所需的结果会被保存在saved_tensors元组中；\n",
    "                                        # 此处仅能保存tensor类型变量，若其余类型变量（Int等），可直接赋予ctx作为成员变量，也可以达到保存效果\n",
    "        return result\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):     # 模型梯度反传\n",
    "        result, = ctx.saved_tensors     # 取出forward中保存的result\n",
    "        return grad_output * result     # 计算梯度并返回\n",
    "\n",
    "\n",
    "# 尝试使用\n",
    "x = torch.tensor([1.], requires_grad=True)  # 需要设置tensor的requires_grad属性为True，才会进行梯度反传\n",
    "ret = Exp.apply(x)                          # 使用apply方法调用自定义autograd function\n",
    "print(ret)                                  # tensor([2.7183], grad_fn=<ExpBackward>)\n",
    "ret.backward()                              # 反传梯度\n",
    "print(x.grad)                               # tensor([2.7183])"
   ],
   "id": "50ea15fb251ebb36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.7183], grad_fn=<ExpBackward>)\n",
      "tensor([2.7183])\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "99fc2bd8c9448ead"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
