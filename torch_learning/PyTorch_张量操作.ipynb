{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-05T02:59:32.788766Z",
     "start_time": "2024-12-05T02:59:31.387335Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[HAMI-core Msg(48810:139736112688000:libvgpu.c:836)]: Initializing.....\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T02:59:35.291409Z",
     "start_time": "2024-12-05T02:59:35.284135Z"
    }
   },
   "cell_type": "code",
   "source": "torch.__version__",
   "id": "a25661e88748016",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0+cu121'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 基本数据处理与计算操作\n",
    "#### 创建Tensor"
   ],
   "id": "8e7335dd4f25c2a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T03:15:35.403637Z",
     "start_time": "2024-12-05T03:15:35.381789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 创建一个2x3的未初始化的Tensor\n",
    "x1 = torch.empty(2, 3)\n",
    "# 创建一个2x3的随机初始化的Tensor\n",
    "x2 = torch.rand(2,3)\n",
    "# 创建一个2x3的long型全0的Tensor\n",
    "x3 = torch.zeros(2, 3, dtype=torch.long)\n",
    "# 为每一个元素以给定的mean和std用高斯分布生成随机数\n",
    "x4 = torch.normal(mean=0.5, std=torch.arange(1., 6.))\n",
    "# randn 服从N(0，1)的正态分布\n",
    "x5 = torch.randn(2, 3)\n",
    "\n",
    "x1, x2, x3, x4, x5"
   ],
   "id": "f98c4fe2f9122de7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0825e-34, 0.0000e+00, 0.0000e+00],\n",
       "         [1.4013e-45, 0.0000e+00, 0.0000e+00]]),\n",
       " tensor([[0.9814, 0.0874, 0.5718],\n",
       "         [0.6293, 0.1877, 0.5226]]),\n",
       " tensor([[0, 0, 0],\n",
       "         [0, 0, 0]]),\n",
       " tensor([ 1.5466, -3.7387, -2.4466,  3.9320,  0.3221]),\n",
       " tensor([[-1.7507, -0.7708, -0.1977],\n",
       "         [ 1.7947,  2.4549, -0.1717]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T05:50:40.808737Z",
     "start_time": "2024-12-05T05:50:40.797425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 根据不同数据类型创建\n",
    "x1 = torch.tensor([[5,5,3], [2,2,5]])\n",
    "x2 = torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "x3 = torch.from_numpy(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "x1, x2, x3"
   ],
   "id": "b88af3712e08564",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5, 5, 3],\n",
       "         [2, 2, 5]]),\n",
       " tensor([[1, 2, 3],\n",
       "         [4, 5, 6]]),\n",
       " tensor([[1, 2, 3],\n",
       "         [4, 5, 6]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T05:52:26.204311Z",
     "start_time": "2024-12-05T05:52:26.185551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([[1,2,3], [4,5,6]])\n",
    "# 返回的tensor默认具有相同的torch.dtype和torch.device \n",
    "x1 = x.new_ones(2, 3, dtype=torch.float64)\n",
    "# 指定新的数据类型\n",
    "x2 = torch.randn_like(x, dtype=torch.float, device='cuda')\n",
    "x1, x2"
   ],
   "id": "efb85eb5dc01eb70",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.]], dtype=torch.float64),\n",
       " tensor([[ 1.2797, -0.0880,  1.2151],\n",
       "         [ 1.1600, -0.2267,  0.4583]], device='cuda:0'))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 操作张量",
   "id": "e0b51474809fc04c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### 算术操作",
   "id": "404c8168a71d8e79"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T06:04:43.481923Z",
     "start_time": "2024-12-05T06:04:43.468071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.ones(2, 3) \n",
    "y = torch.eye(2, 3)\n",
    "# 加法形式1\n",
    "s1 = x + y\n",
    "# 加法形式2\n",
    "s2 = torch.add(x, y)\n",
    "# 加法形式3，inplace(原地操作)，原值修改\n",
    "s3 = y.add_(x)\n",
    "\n",
    "x, y, s1, s2, s3, y"
   ],
   "id": "c38d174958713b62",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[2., 1., 1.],\n",
       "         [1., 2., 1.]]),\n",
       " tensor([[2., 1., 1.],\n",
       "         [1., 2., 1.]]),\n",
       " tensor([[2., 1., 1.],\n",
       "         [1., 2., 1.]]),\n",
       " tensor([[2., 1., 1.],\n",
       "         [1., 2., 1.]]),\n",
       " tensor([[2., 1., 1.],\n",
       "         [1., 2., 1.]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### 索引\n",
    "索引出来的结果与原数据`共享内存`，也即修改一个，另一个会跟着修改。"
   ],
   "id": "e9e8ea08923b768c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T06:10:21.218064Z",
     "start_time": "2024-12-05T06:10:21.203136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.ones(2, 3, device='cuda') \n",
    "y = x[0, :]\n",
    "y += 1\n",
    "\n",
    "x, y, x[0, :]"
   ],
   "id": "952e713dea06fdb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 2., 2.],\n",
       "         [1., 1., 1.]], device='cuda:0'),\n",
       " tensor([2., 2., 2.], device='cuda:0'),\n",
       " tensor([2., 2., 2.], device='cuda:0'))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### 切片 index_select\n",
    "沿着指定维度对输入进行切片，取index中指定的相应项(index 为一个 LongTensor)，然后返回到一个新的张量， 返回的张量与原始张量 Tensor 有相同的维度(在指定轴上)。\n",
    "\n",
    "注意： `返回的张量不与原始张量共享内存空间`。<br>\n",
    "\n",
    "参数:<br>\n",
    "input (Tensor)         – 输入张量<br>\n",
    "dim (int)              – 索引的轴<br>\n",
    "index (LongTensor)     – 包含索引下标的一维张量<br>\n",
    "out (Tensor, optional) – 目标张量<br>"
   ],
   "id": "4c83e29022cd4abd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T06:28:21.135491Z",
     "start_time": "2024-12-05T06:28:21.119048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.randn(3, 4)\n",
    "indices = torch.LongTensor([0, 2])\n",
    "y = torch.index_select(x, 0, indices)\n",
    "x, indices, y"
   ],
   "id": "be273576e54333f9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1912,  0.1199, -0.0856, -1.8215],\n",
       "         [ 0.2932,  0.9630,  0.3461, -0.7105],\n",
       "         [-1.2408, -1.6029,  0.8402,  0.3389]]),\n",
       " tensor([0, 2]),\n",
       " tensor([[ 0.1912,  0.1199, -0.0856, -1.8215],\n",
       "         [-1.2408, -1.6029,  0.8402,  0.3389]]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### 改变形状 view/reshape\n",
    "1. `view()`<br>\n",
    "注意view()**返回的新Tensor与源Tensor虽然可能有不同的size**，但`共享data`。<br>\n",
    "即更改其中的一个，另外一个也会跟着改变。(顾名思义，view仅仅是改变了对这个张量的 观察角度，内部数据并未改变)\n",
    "2. `reshape()` 和 `clone()`<br> \n",
    "Pytorch还提供了一个 reshape() 方法可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。<br> \n",
    "我们推荐先用 clone() 创造一个副本然后再使用view()。"
   ],
   "id": "638e71766e4e379d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T06:37:26.690979Z",
     "start_time": "2024-12-05T06:37:26.674120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.ones(2, 4)\n",
    "# 一个tensor必须是连续的contiguous()才能被查看。\n",
    "# 一开始不加contiguous()，报 “view size is not compatible ... ” 错误\n",
    "y1 = x.view(8)\n",
    "y2 = x.view(-1, 8) # -1所指的维度可以根据其他维度的值推出来\n",
    "\n",
    "x, y1, y2, x.size(), y1.size(), y2.size()"
   ],
   "id": "228a589372df84e0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " tensor([[1., 1., 1., 1., 1., 1., 1., 1.]]),\n",
       " torch.Size([2, 4]),\n",
       " torch.Size([8]),\n",
       " torch.Size([1, 8]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T06:37:27.557078Z",
     "start_time": "2024-12-05T06:37:27.548856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x += 1\n",
    "print(x)\n",
    "print(y1) # 也加了1"
   ],
   "id": "ff5073877939dce0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.]])\n",
      "tensor([2., 2., 2., 2., 2., 2., 2., 2.])\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T06:40:50.680881Z",
     "start_time": "2024-12-05T06:40:50.665059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# torch.view()会改变原始张量，但是很多情况下，我们希望原始张量和变换后的张量互相不影响。为为了使创建的张量和原始张量不共享内存，我们需要使用第二种方法torch.reshape()， 同样可以改变张量的形状，但是此函数并不能保证返回的是其拷贝值，所以官方不推荐使用。推荐的方法是我们先用 `clone()` 创造一个张量副本然后再使用 `torch.view()`进行函数维度变换。<br>\n",
    "# 注：使用 clone() 还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源 Tensor 。 \n",
    "\n",
    "x1 = x.clone()\n",
    "y3 = x.view(-1)\n",
    "x1 += 1\n",
    "\n",
    "x1, y3"
   ],
   "id": "2966e0478d013f3d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3.]]),\n",
       " tensor([2., 2., 2., 2., 2., 2., 2., 2.]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### gather\n",
    "`torch.gather(input, dim, index, out=None)`<br>\n",
    "官方定义：沿给定轴dim，将输入索引张量index指定位置的值进行聚合。<br>\n",
    "通俗理解：给定轴dim，在input中，根据index指定的下标，选择元素重组成一个新的tensor，最后输出的out与index的size是一样的。<br>\n",
    "\n",
    "对一个3维张量，输出可以定义为<br>\n",
    "\n",
    "```\n",
    "out[i][j][k] = tensor[index[i][j][k]][j][k]] # dim=0\n",
    "out[i][j][k] = tensor[i][index[i][j][k]][k]] # dim=1\n",
    "out[i][j][k] = tensor[i][j][index[i][j][k]]] # dim=3\n",
    "```"
   ],
   "id": "4c4092d912b18775"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T06:44:34.623357Z",
     "start_time": "2024-12-05T06:44:34.607203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_input = torch.tensor([[0.9, 0, 0], [0, 0.6, 0],[0, 0.7, 0]])\n",
    "index = torch.tensor([0, 1, 1])\n",
    "_input, index, index.view(-1, 1)"
   ],
   "id": "c461832c1b04d76f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.6000, 0.0000],\n",
       "         [0.0000, 0.7000, 0.0000]]),\n",
       " tensor([0, 1, 1]),\n",
       " tensor([[0],\n",
       "         [1],\n",
       "         [1]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.9000],\n        [0.6000],\n        [0.7000]])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30,
   "source": [
    "_input.gather(dim=1, index=index.view(-1, 1))\n",
    "# dim=1，表示的是在第二维度上操作。\n",
    "# 在index中，[0]表示第一行对应元素的下标，即[0.9]\n",
    "#           [1]表示第二行对应元素的下标，即[0.6]\n",
    "#           [1]表示第三行对应元素的下标，即[0.7]\n"
   ],
   "id": "2dd910f8efdeb599"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 广播机制\n",
    "当我们对两个形状不同的Tensor按元素运算时，可能会触发`广播(broadcasting)机制`。 <br>\n",
    "先适当复制元素使这两个Tensor形状相同后再按元素运算。"
   ],
   "id": "70802a520b875fca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T06:51:29.369464Z",
     "start_time": "2024-12-05T06:51:29.354588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# x 中第一行的2个 元素被广播(复制)到了第二行和第三行\n",
    "# y 中第一列的3个元素被广播(复制)到 了第二列\n",
    "x = torch.arange(1, 3).view(1, 2)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "x, y, x+y"
   ],
   "id": "b84ba14b9b1fd304",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2]]),\n",
       " tensor([[1],\n",
       "         [2],\n",
       "         [3]]),\n",
       " tensor([[2, 3],\n",
       "         [3, 4],\n",
       "         [4, 5]]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tensor和NumPy相互转换\n",
    "+ `numpy()`和`from_numpy()` <br>\n",
    "这两个函数所产生的的Tensor和NumPy中的数组共享相同的内存(所以他们之间的转换很快)，改变其中一个时另一个也会改变!\n",
    "+ `torch.tensor()` <br>\n",
    "进行**数据拷贝**，所以返回的Tensor和原来 的数据不再共享内存。"
   ],
   "id": "a074be98b2d79f77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Tensor转NumPy数组",
   "id": "476d40c00e3b2f5c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T06:57:00.723692Z",
     "start_time": "2024-12-05T06:57:00.708576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tensor转NumPy数组\n",
    "x = torch.ones(3)\n",
    "y = x.numpy()\n",
    "x += 1\n",
    "y += 1\n",
    "print(x, y)\n",
    "\n",
    "# NumPy数组转 Tensor\n",
    "x = np.ones(3)\n",
    "y = torch.from_numpy(x)\n",
    "x += 1\n",
    "y += 1\n",
    "print(x, y)"
   ],
   "id": "43545e1b02a013db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3.]) [3. 3. 3.]\n",
      "[3. 3. 3.] tensor([3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T06:58:12.393425Z",
     "start_time": "2024-12-05T06:58:12.380853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 使用torch.tensor()将NumPy数组转换成Tensor(不再共享内存)\n",
    "x = np.ones(3)\n",
    "y = torch.tensor(x)\n",
    "x += 1\n",
    "print(x, y)"
   ],
   "id": "d70a731be351d2d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2.] tensor([1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tensor on GPU\n",
    "+ `to()`<br>\n",
    "将Tensor在CPU和GPU(需要硬件支持)之间相互移动。"
   ],
   "id": "6b5fffe819d9fb70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 自动求梯度\n",
    "+ `autograd 包`<br>\n",
    "根据输入和**前向传播**过程自动构建计算图，并执行**反向传播**。\n",
    "+ `Tensor`是autograd包的核心类<br>\n",
    "将其属性`.requires_grad`设置为True，它将开始追踪在其上的所有操作(这样就可以利用链式法则进行梯度传播了)。<br>\n",
    "完成计算后，可以调用`.backward()`来完成所有梯度计算。此Tensor的梯度将累积到.grad属性中。<br>\n",
    "如果不想要被继续追踪，可以调用`.detach()`将其从追踪记录中分离出来，这样就可以防止将来的计算被追踪，这样梯度就传不过去了。<br>\n",
    "此外，还可以用`with torch.no_grad()`将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用，因为在评估模型时，我们并不需要计算 可训练参数(requires_grad=True)的梯度。\n",
    "+ `Function`是另外一个很重要的类。<br>\n",
    "Tensor和Function互相结合就可以构建一个记录有整个计算过程的**有向无环图(DAG)**。<br>\n",
    "每个Tensor都有一个.grad_fn属性，该属性即创建该Tensor的Function, 就是说该Tensor是不是通过某些运算得到的，若是，则grad_fn返回一个与这些运算相关的对象，否则是None。"
   ],
   "id": "24e03c74202568ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Tensor",
   "id": "599d6a37ba587d76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[1., 1.],\n         [1., 1.]], requires_grad=True),\n None)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39,
   "source": [
    "# 创建一个Tensor并设置requires_grad=True:\n",
    "# x是直接创建的，所以它没有grad_fn\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "x, x.grad_fn "
   ],
   "id": "b0554114476ba53b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[3., 3.],\n         [3., 3.]], grad_fn=<AddBackward0>),\n <AddBackward0 at 0x13503b2e8>)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40,
   "source": [
    "# y 是通过一个加法操作创建的，所以它有一个为<AddBackward>的grad_fn\n",
    "y = x + 2\n",
    "y, y.grad_fn"
   ],
   "id": "89bb0a72efea8335"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[27., 27.],\n         [27., 27.]], grad_fn=<MulBackward0>),\n tensor(27., grad_fn=<MeanBackward0>))"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41,
   "source": [
    "z = y * y * 3 \n",
    "out = z.mean()\n",
    "z, out"
   ],
   "id": "4aa4b93192fde4e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42,
   "source": [
    "# 通过.requires_grad_()来用in-place的方式改变requires_grad属性\n",
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "a.requires_grad"
   ],
   "id": "2c929a9704c4a359"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43,
   "source": [
    "a.requires_grad_(True)\n",
    "a.requires_grad"
   ],
   "id": "4dbed48b932de871"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<SumBackward0 at 0x13503b6d8>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44,
   "source": [
    "b = (a * a).sum()\n",
    "b.grad_fn"
   ],
   "id": "893aaf15fc641fa4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### 梯度",
   "id": "7ae213362d3537b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[4.5000, 4.5000],\n        [4.5000, 4.5000]])"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45,
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "out.backward()  # 等价于 out.backward(torch.tensor(1.))\n",
    "x.grad"
   ],
   "id": "19844e419c1fc46a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "令out为o，因为\n",
    "$$\n",
    "O = \\frac 1 4 \\sum_i^4 z_i = \\frac 1 4 \\sum_i^4 3y_i^2 = \\frac 1 4 \\sum_i^4 3(x_i+2)^2\n",
    "$$\n",
    "所以<br>\n",
    "$$\n",
    "\\frac{\\partial{O}}{\\partial{x_i}}|_{x_i=1} = \\frac 9 2 = 4.5\n",
    "$$"
   ],
   "id": "5661f3929aa07792"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[5.5000, 5.5000],\n        [5.5000, 5.5000]])"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46,
   "source": [
    "# 注意grad是累加的\n",
    "out2 = x.sum()\n",
    "out2.backward()      # 梯度未清零，累加梯度\n",
    "x.grad"
   ],
   "id": "1dd4fc866aba303c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1.],\n        [1., 1.]])"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47,
   "source": [
    "out3 = x.sum()\n",
    "x.grad.data.zero_()\n",
    "out3.backward()      # 梯度清零后，x的梯度为1\n",
    "x.grad"
   ],
   "id": "b18f273eff49c1ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### 反向传播\n",
    "`y.backward()`<br>\n",
    "+ 如果 $y$ 是**标量**，则不需要为backward()传入任何参数。<br>\n",
    "+ 如果 $y$ 是**向量**，需要传入一个与y同形的Tensor。<br>\n",
    "这样为了避免向量(甚至更高维张量) 对张量求导，因此转换成标量对张量进行求导。<br>\n",
    "`不允许张量对张量求导`，只允许标量对张量求导，求导结果是和自变量同形的张量。<br>\n",
    "必要时我们要把张量通过将所有张量的元素`加权求和`的方式转换为标量。<br>\n",
    "\n",
    "假设 $𝒚$ 由自变量 $𝒙$ 计算而来，$𝒘$ 是和 $𝒚$ 同形的张量则 $𝐲. 𝐛𝐚𝐜𝐤𝐰𝐚𝐫𝐝(𝒘)$ 的含义是: <br>\n",
    "先计算 $𝒍$ = 𝐭𝐨𝐫𝐜𝐡.𝐬𝐮𝐦($𝒚$ ∗ $𝒘$)，因此 𝒍 是一个标量，然后我们再求 𝒍 对自变量 𝒙 的导数。"
   ],
   "id": "973132663b5a0e47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([1., 2., 3., 4.], requires_grad=True),\n tensor([2., 4., 6., 8.], grad_fn=<MulBackward0>),\n tensor([[2., 4.],\n         [6., 8.]], grad_fn=<ViewBackward>))"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48,
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
    "y = 2 * x\n",
    "z = y.view(2, 2)\n",
    "x, y, z"
   ],
   "id": "4a44579cf37b8c67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "此时 𝒛 不是一个标量，所以在调用backward()时需要传入一个和 𝒛 同形的权重向量进行加权求和来得到一个标量。",
   "id": "d442a87f562053e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2.0000, 0.2000, 0.0200, 0.0020])"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49,
   "source": [
    "v = torch.tensor([[1.0, 0.1], [0.01, 0.001]], dtype=torch.float) \n",
    "z.backward(v)\n",
    "x.grad"
   ],
   "id": "98b333675f859f71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### .detach()和.data\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/67184419"
   ],
   "id": "8458bef2dee0f6b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 49,
   "source": "",
   "id": "f44e76111711fa8a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
