
## 一、微积分
### 1.1 导数和微分、偏导数、梯度
##### 导数和微分

假设有一个函数 $f: \mathbb{R} \rightarrow \mathbb{R}$ ，其输入和输出都是标量，如果 $f$ 的*导数*存在，这个极限被定义为：

$$
f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h}.
$$

如果 $f'(a)$ 存在，则称 $f$ 在 $a$ 处是**可微**（differentiable）的。<br>
如果 $f$ 在一个区间内的每个数上都是可微的，则此函数在此区间中是可微的。<br>
以下表达式是等价的：

$$
f'(x) = y' = \frac{dy}{dx} = \frac{df}{dx} = \frac{d}{dx} f(x) = Df(x) = D_x f(x),
$$

##### 偏导数

设多元函数 $y = f(x_1, x_2, \ldots, x_n)$ 是一个具有 $n$ 个变量的函数。 $y$ 关于第 $i$ 个参数 $x_i$ 的**偏导数**（partial derivative）为：

$$
\frac{\partial y}{\partial x_i} = \lim_{h \rightarrow 0} \frac{f(x_1, \ldots, x_{i-1}, x_i+h, x_{i+1}, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}.
$$

为了计算 $\frac{\partial y}{\partial x_i}$ ，我们可以简单地将 $x_1 , \ldots , x_{i-1} , x_{i+1} , \ldots , x_n$ 看作常数，并计算 $y$ 关于 $x_i$ 的导数。对于偏导数的表示，以下是等价的：

$$
\frac{\partial y}{\partial x_i} = \frac{\partial f}{\partial x_i} = f_{x_i} = f_i = D_i f = D_{x_i} f.
$$

##### 梯度

我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的**梯度**（gradient）向量。具体而言，设函数 $f:\mathbb{R}^n\rightarrow\mathbb{R}$ 的输入是一个 $n$ 维向量 $\mathbf{x}=[x_1,x_2,\ldots,x_n]^\top$ ，并且输出是一个标量。函数 $f(\mathbf{x})$ 相对于 $\mathbf{x}$ 的梯度是一个包含 $n$ 个偏导数的向量:

$$
\nabla_{\mathbf{x}} f(\mathbf{x}) = \bigg[\frac{\partial f(\mathbf{x})}{\partial x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_n}\bigg]^\top,
$$
其中 $\nabla_{\mathbf{x}} f(\mathbf{x})$ 通常在没有歧义时被 $\nabla f(\mathbf{x})$ 取代。



### 1.2 矩阵微分



假设 $\mathbf{x}$ 为 $n$ 维向量，在微分多元函数时经常使用以下规则:

* 对于所有 $\mathbf{A} \in \mathbb{R}^{m \times n}$ ，都有 $\nabla_{\mathbf{x}} \mathbf{A} \mathbf{x} = \mathbf{A}^\top$
* 对于所有 $\mathbf{A} \in \mathbb{R}^{n \times m}$ ，都有 $\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A}  = \mathbf{A}$
* 对于所有 $\mathbf{A} \in \mathbb{R}^{n \times n}$ ，都有 $\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} \mathbf{x}  = (\mathbf{A} + \mathbf{A}^\top)\mathbf{x}$
* $\nabla_{\mathbf{x}} \|\mathbf{x} \|^2 = \nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{x} = 2\mathbf{x}$

同样，对于任何矩阵 $\mathbf{X}$ ，都有 $\nabla_{\mathbf{X}} \|\mathbf{X} \|_F^2 = 2\mathbf{X}$ 。
正如我们之后将看到的，梯度对于设计深度学习中的优化算法有很大用处。



1. 《PyTorch深度学习实践》 <br>
[《PyTorch深度学习实践》课程](https://liuii.github.io/post/pytorch-tutorials/)<br>
[《PyTorch深度学习实践》笔记](https://github.com/MLNLP-World/Pytorch-Deep-Learning-Practice-Notes/tree/main)<br>
[《PyTorch深度学习实践》代码](https://github.com/DelinQu/pytorch-prev/tree/master)<br>

2. 《动手深度学习v2.0》 <br>
[在线课程](https://courses.d2l.ai/zh-v2/)<br>
[视频地址](https://www.bilibili.com/video/BV1if4y147hS/?spm_id_from=333.999.0.0)<br>
[在线书籍](https://zh.d2l.ai/index.html)<br>
[项目代码](https://github.com/d2l-ai/d2l-zh)<br>
[学习笔记](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/tree/main)<br>

[矩阵求导入门学习路线参考](https://zhuanlan.zhihu.com/p/343299481)<br>
[矩阵求导与实例](https://blog.csdn.net/young_gy/article/details/50008953)<br>
[机器学习 标量、向量、矩阵的求导 PyTorch自动求导](https://blog.csdn.net/qq_45523675/article/details/127503367)
[机器学习：多元线性回归中矩阵求导方法](https://btoai.com/p/300)
[刘泽平-机器学习中的矩阵向量求导(一) 求导定义与求导布局](https://www.cnblogs.com/pinard/p/10750718.html)
[Autograd (1)：PyTorch 自动一阶求导在标量、向量、矩阵、张量运算中的定义](https://ajz34.readthedocs.io/zh-cn/latest/ML_Notes/Autograd_Series/Autograd_TensorContract.html)


## 参考引用

[1] [《PyTorch实用教程》（第二版）](https://github.com/TingsongYu/PyTorch-Tutorial-2nd/releases/tag/v1.0.0)<br>
[2] [《深入浅出PyTorch》](https://github.com/datawhalechina/thorough-pytorch)<br>
[3] [PyTorch中文文档](https://www.bookstack.cn/read/PyTorch-cn/README.md)<br>
[4] [一览 Pytorch框架](https://zhuanlan.zhihu.com/p/334788042)<br>
[5] [PyTorch的核心模块介绍](https://blog.csdn.net/weixin_38566632/article/details/135442466)<br>
[6] [PyTorch 2.4.0 版本发布](https://pytorch.org/get-started/previous-versions/#v240)<br>
[7] [20天吃掉那只Pytorch](https://github.com/lyhue1991/eat_pytorch_in_20_days/tree/master)<br>
[8] [PyTorch 源码解读之 torch.autograd：梯度计算详解](https://zhuanlan.zhihu.com/p/321449610)<br>
[9] [《动手学深度学习v2》](https://zh-v2.d2l.ai/)<br>
[9] [动手学深度学习在线课程](https://courses.d2l.ai/zh-v2/)<br>
