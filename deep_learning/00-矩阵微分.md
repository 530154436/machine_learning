
## 一、微积分
### 1.1 基本概念
##### 导数和微分

假设有一个函数 $f: \mathbb{R} \rightarrow \mathbb{R}$ ，其输入和输出都是标量，如果 $f$ 的*导数*存在，这个极限被定义为：

$$
f'(x) = \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h}.
$$

如果 $f'(a)$ 存在，则称 $f$ 在 $a$ 处是**可微**（differentiable）的。<br>
如果 $f$ 在一个区间内的每个数上都是可微的，则此函数在此区间中是可微的。<br>
以下表达式是等价的：

$$
f'(x) = y' = \frac{dy}{dx} = \frac{df}{dx} = \frac{d}{dx} f(x) = Df(x) = D_x f(x),
$$

##### 偏导数

设多元函数 $y = f(x_1, x_2, \ldots, x_n)$ 是一个具有 $n$ 个变量的函数。 $y$ 关于第 $i$ 个参数 $x_i$ 的**偏导数**（partial derivative）为：

$$
\frac{\partial y}{\partial x_i} = \lim_{h \rightarrow 0} \frac{f(x_1, \ldots, x_{i-1}, x_i+h, x_{i+1}, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}.
$$

为了计算 $\frac{\partial y}{\partial x_i}$ ，我们可以简单地将 $x_1 , \ldots , x_{i-1} , x_{i+1} , \ldots , x_n$ 看作常数，并计算 $y$ 关于 $x_i$ 的导数。对于偏导数的表示，以下是等价的：

$$
\frac{\partial y}{\partial x_i} = \frac{\partial f}{\partial x_i} = f_{x_i} = f_i = D_i f = D_{x_i} f.
$$

##### 梯度

我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的**梯度**（gradient）向量。具体而言，设函数 $f:\mathbb{R}^n\rightarrow\mathbb{R}$ 的输入是一个 $n$ 维向量 $\mathbf{x}=[x_1,x_2,\ldots,x_n]^\top$ ，并且输出是一个标量。函数 $f(\mathbf{x})$ 相对于 $\mathbf{x}$ 的梯度是一个包含 $n$ 个偏导数的向量:

$$
\nabla_{\mathbf{x}} f(\mathbf{x}) = \bigg[\frac{\partial f(\mathbf{x})}{\partial x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_n}\bigg]^\top,
$$

其中 $\nabla_{\mathbf{x}} f(\mathbf{x})$ 通常在没有歧义时被 $\nabla f(\mathbf{x})$ 取代。

##### 雅可比（Jacobi）矩阵
假设某函数 $f : \mathbb{R}^n \to \mathbb{R}^m$ ，从 $x \in \mathbb{R}^n$ 映射到向量 $f(x) \in \mathbb{R}^m$ ，其雅可比矩阵是一个 $m \times n$ 的矩阵（分子布局）。换句话说，它表示从 $\mathbb{R}^n$ 到 $\mathbb{R}^m$ 的线性映射，其重要意义在于它表现了一个多变量向量函数的最佳线性逼近。

$$
\mathbf{J} =
\begin{bmatrix}
\frac{\partial \mathbf{f}}{\partial x_1} & \cdots & \frac{\partial \mathbf{f}}{\partial x_n}
\end{bmatrix}
=
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n}
\end{bmatrix}
$$

##### 黑塞（Hessan）矩阵

假设有一实值函数 $f(x_1, x_2, \dots , x_n)$，如果 $f$ 的所有二阶偏导数都存在并在定义域内连续，那么函数 $f$ 的黑塞矩阵为是一个 $n \times n$ 的方阵，即

$$
H = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1 \partial x_1} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n \partial x_n}
\end{bmatrix}
$$


### 1.2 符号定义和求导布局
#### 1.2.1  符号定义
所谓**向量对标量的求导**，其实就是向量里的每个分量分别对标量求导，最后把求导的结果排列在一起，按一个向量表示而已。类似的结论也存在于`标量对向量的求导`，`向量对向量的求导`，`向量对矩阵的求导`，`矩阵对向量的求导`，以及`矩阵对矩阵的求导`等。总而言之，所谓的向量矩阵求导本质上就是多元函数求导，仅仅是把函数的自变量、因变量以及标量求导的结果排列成了向量矩阵的形式，方便表达与计算、更加简洁而已。<br>

符号定义：
+ 求导的自变量<br>
  $x$ 表示标量， $\mathbf{x}$ 表示 $n$ 维向量， $\mathbf{X}$ 表示$m \times n$维度的矩阵
+ 求导的因变量<br>
  $y$ 表示标量， $\mathbf{y}$ 表示 $m$ 维向量， $\mathbf{Y}$ 表示 $p \times q$ 维度的矩阵。<br>

根据求导的自变量和因变量是标量，向量还是矩阵，我们有9种可能的矩阵求导定义，如下：

| 自变量\因变量| 标量$y$ | 向量$\mathbf{y}$ | 矩阵$\mathbf{Y}$ |
| :---------: | :--------------: | :---------------: | :----------: |
| 标量$x$          | $\frac{\partial y}{\partial x}$          | $\frac{\partial \mathbf{y}}{\partial x}$          | $\frac{\partial \mathbf{Y}}{\partial x}$          |
| 向量$\mathbf{x}$ | $\frac{\partial y}{\partial \mathbf{x}}$ | $\frac{\partial \mathbf{y}}{\partial \mathbf{x}}$ | $\frac{\partial \mathbf{Y}}{\partial \mathbf{x}}$ |
| 矩阵$\mathbf{X}$ | $\frac{\partial y}{\partial \mathbf{X}}$ | $\frac{\partial \mathbf{y}}{\partial \mathbf{X}}$ | $\frac{\partial \mathbf{Y}}{\partial \mathbf{X}}$ |

#### 1.2.2 求导布局

为了解决矩阵向量求导的结果不唯一，我们引入求导布局。最基本的求导布局有两个：**分子布局**(numerator layout)和**分母布局**(denominator layout )。

+ 对于分子布局来说，求导结果的维度以分子为主。<br>
  $m$ 维列向量 $\mathbf{y}$ 对标量 $x$ 求导： $\frac{\partial \mathbf{y}}{\partial x}$ 是一个 $m$ 维列向量。<br>
  $m$ 维行向量 $\mathbf{y}$ 对标量 $x$ 求导： $\frac{\partial \mathbf{y}}{\partial x}$ 是一个 $m$ 维行向量。<br>
  $m$ 维列向量 $\mathbf{y}$ 对 $n$ 维列向量 $\mathbf{x}$ 求导：$\frac{\partial  \mathbf{y}}{\partial \mathbf{x}} $ 矩阵的第一个维度以分子为准，即结果是一个 $m \times n$ 的矩阵。

+ 对于分母布局来说，我们求导结果的维度以分母为主。<br>
  $m$ 维列向量 $\mathbf{y}$ 对标量 $x$ 求导： $\frac{\partial \mathbf{y}}{\partial x}$ 是一个 $m$ 维行向量。<br>
  $m$ 维行向量 $\mathbf{y}$ 对标量 $x$ 求导： $\frac{\partial \mathbf{y}}{\partial x}$ 是一个 $m$ 维列向量。<br>
  $m$ 维列向量 $\mathbf{y}$ 对 $n$ 维列向量 $\mathbf{x}$ 求导：$\frac{\partial  \mathbf{y}}{\partial \mathbf{x}} $ 矩阵的第一个维度以分母为准，即结果是一个 $n \times m$ 的矩阵。

一般来说会使用一种叫`混合布局`的思路，即如果是向量或者矩阵对标量求导，则使用分子布局为准；如果是标量对向量或者矩阵求导，则以分母布局为准。对于向量对对向量求导，这里以分子布局的雅克比矩阵为主。

| 自变量\因变量                 | 标量$y$ | $m$ 维列向量$\mathbf{y}$                                                                                       | 矩阵$\mathbf{Y}$ |
|-------------------------| --- |------------------------------------------------------------------------------------------------------------| --- |
| **标量$x$**               | / | $\frac{\partial \mathbf{y}}{\partial x}$<br>分子布局：m维列向量（默认布局）<br>分母布局：m维行向量                                 | $\frac{\partial \mathbf{Y}}{\partial x}$<br>分子布局：$p \times q$矩阵（默认布局）<br>分母布局：$q \times p$矩阵 |
| **$n$ 列向量$\mathbf{x}$** | $\frac{\partial y}{\partial \mathbf{x}}$<br>分子布局：n维行向量<br>分母布局：n维列向量（默认布局） | $\frac{\partial \mathbf{y}}{\partial \mathbf{x}}$<br>分子布局：$m \times n$雅克比矩阵（默认布局）<br>分母布局：$n \times m$梯度矩阵 | / |
| **矩阵$\mathbf{X}$**      | $\frac{\partial y}{\partial \mathbf{X}}$<br>分子布局：$n \times m$矩阵<br>分母布局：$m \times n$矩阵（默认布局） | /                                                                                                          | / |

### 1.3 矩阵微分


[机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法](https://www.cnblogs.com/pinard/p/10773942.html)












假设 $\mathbf{x}$ 为 $n$ 维向量，在微分多元函数时经常使用以下规则:

* 对于所有 $\mathbf{A} \in \mathbb{R}^{m \times n}$ ，都有 $\nabla_{\mathbf{x}} \mathbf{A} \mathbf{x} = \mathbf{A}^\top$
* 对于所有 $\mathbf{A} \in \mathbb{R}^{n \times m}$ ，都有 $\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A}  = \mathbf{A}$
* 对于所有 $\mathbf{A} \in \mathbb{R}^{n \times n}$ ，都有 $\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} \mathbf{x}  = (\mathbf{A} + \mathbf{A}^\top)\mathbf{x}$
* $\nabla_{\mathbf{x}} \|\mathbf{x} \|^2 = \nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{x} = 2\mathbf{x}$

同样，对于任何矩阵 $\mathbf{X}$ ，都有 $\nabla_{\mathbf{X}} \|\mathbf{X} \|_F^2 = 2\mathbf{X}$ 。
正如我们之后将看到的，梯度对于设计深度学习中的优化算法有很大用处。

1. 邱锡鹏《神经网络与深度学习》 <br>
[在线课程](https://nndl.github.io/)<br>
[视频地址](https://www.bilibili.com/video/BV1p1421k7MC)<br>
[项目代码](https://github.com/nndl/practice-in-paddle/)<br>

2. 李沐《动手深度学习v2.0》 <br>
[在线课程](https://courses.d2l.ai/zh-v2/)<br>
[视频地址](https://www.bilibili.com/video/BV1if4y147hS/?spm_id_from=333.999.0.0)<br>
[在线书籍](https://zh.d2l.ai/index.html)<br>
[项目代码](https://github.com/d2l-ai/d2l-zh)<br>
[学习笔记](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/tree/main)<br>

[矩阵求导入门学习路线参考](https://zhuanlan.zhihu.com/p/343299481)<br>
[矩阵求导与实例](https://blog.csdn.net/young_gy/article/details/50008953)<br>
[机器学习 标量、向量、矩阵的求导 PyTorch自动求导](https://blog.csdn.net/qq_45523675/article/details/127503367)
[机器学习：多元线性回归中矩阵求导方法](https://btoai.com/p/300)
[Autograd (1)：PyTorch 自动一阶求导在标量、向量、矩阵、张量运算中的定义](https://ajz34.readthedocs.io/zh-cn/latest/ML_Notes/Autograd_Series/Autograd_TensorContract.html)


## 参考引用

[1]  [《动手学深度学习v2》](https://zh-v2.d2l.ai/)<br>
[2] [刘泽平-机器学习中的矩阵向量求导(一) 求导定义与求导布局](https://www.cnblogs.com/pinard/p/10750718.html)<br>




[《深入浅出PyTorch》](https://github.com/datawhalechina/thorough-pytorch)<br>
[3] [PyTorch中文文档](https://www.bookstack.cn/read/PyTorch-cn/README.md)<br>
[4] [一览 Pytorch框架](https://zhuanlan.zhihu.com/p/334788042)<br>
[5] [PyTorch的核心模块介绍](https://blog.csdn.net/weixin_38566632/article/details/135442466)<br>
[6] [PyTorch 2.4.0 版本发布](https://pytorch.org/get-started/previous-versions/#v240)<br>
[7] [20天吃掉那只Pytorch](https://github.com/lyhue1991/eat_pytorch_in_20_days/tree/master)<br>
[8] [PyTorch 源码解读之 torch.autograd：梯度计算详解](https://zhuanlan.zhihu.com/p/321449610)<br>
[9][《PyTorch实用教程》（第二版）](https://github.com/TingsongYu/PyTorch-Tutorial-2nd/releases/tag/v1.0.0)<br>
[9] [动手学深度学习在线课程](https://courses.d2l.ai/zh-v2/)<br>
