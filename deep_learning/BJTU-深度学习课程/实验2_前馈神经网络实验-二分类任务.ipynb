{
 "cells": [
  {
   "cell_type": "code",
   "id": "automatic-superintendent",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-11-30T06:10:24.273355Z",
     "start_time": "2024-11-30T06:10:21.708711Z"
    }
   },
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def train_epoch(data_loader: Data.DataLoader, net: nn.Module,\n",
    "                loss_func, optimizer: torch.optim.Optimizer,\n",
    "                device='cpu'):\n",
    "    \"\"\"\n",
    "    训练迭代一次\n",
    "    :param data_loader: 生成器 (x, y)\n",
    "    :param net:         模型\n",
    "    :param loss_func:   损失函数\n",
    "    :param optimizer:   优化器\n",
    "    :param device:      设备\n",
    "    :return:    loss, acc\n",
    "    \"\"\"\n",
    "    net.train()\n",
    "    train_batch_num = len(data_loader)\n",
    "\n",
    "    # 一次迭代中的 Loss、正确样本数、总样本数\n",
    "    total_loss, correct, sample_num = 0, 0, 0\n",
    "\n",
    "    for batch_num, (x, y_true) in enumerate(data_loader):\n",
    "\n",
    "        # 将数据放入指定的设备\n",
    "        x = x.to(device).float()\n",
    "        y_true = y_true.to(device).long()\n",
    "\n",
    "        # 计算损失\n",
    "        y_hat: torch.Tensor = net(x)\n",
    "        loss = loss_func(y_hat, y_true)\n",
    "\n",
    "        # 取概率最大的类别索引\n",
    "        y_true: torch.Tensor = y_true.view(-1)\n",
    "        y_hat = y_hat.argmax(dim=1)\n",
    "\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad() #清空梯度\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (y_true == y_hat).float().sum().item()\n",
    "        sample_num += len(y_true)\n",
    "\n",
    "    loss = total_loss / train_batch_num\n",
    "    acc = correct / sample_num\n",
    "\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "def test_epoch(data_loader: Data.DataLoader,\n",
    "               net: nn.Module, loss_func, device='cpu'):\n",
    "    \"\"\"\n",
    "    测试函数迭代一次\n",
    "    :param data_loader: 生成器 (x, y)\n",
    "    :param net:         模型\n",
    "    :param loss_func:   损失函数\n",
    "    :param device:      设备\n",
    "    :return:    loss, acc\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    train_batch_num = len(data_loader)\n",
    "\n",
    "    # 一次迭代中的 Loss、正确样本数、总样本数\n",
    "    total_loss, correct, sample_num = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch_num, (x, y_true) in enumerate(data_loader):\n",
    "\n",
    "            # 将数据放入指定的设备\n",
    "            x = x.to(device).float()\n",
    "            y_true = y_true.to(device).long()\n",
    "\n",
    "            # 计算损失\n",
    "            y_hat: torch.Tensor = net(x)\n",
    "            loss = loss_func(y_hat, y_true)\n",
    "\n",
    "            # 取概率最大的类别索引\n",
    "            y_true: torch.Tensor = y_true.view(-1)\n",
    "            y_hat = y_hat.argmax(dim=1)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (y_true == y_hat).float().sum().item()\n",
    "            sample_num += len(y_true)\n",
    "\n",
    "    loss = total_loss / train_batch_num\n",
    "    acc = correct / sample_num\n",
    "\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "def plot_loss_accuracy(train_loss_list, train_acc_list,\n",
    "                       test_loss_list, test_acc_list, info=''):\n",
    "    \"\"\"\n",
    "    绘制 训练集和测试集正确率、损失值 的图形\n",
    "    \"\"\"\n",
    "    x = np.linspace(0, len(train_loss_list), len(train_loss_list))\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(10, 6)\n",
    "    if info:\n",
    "        info = info + ' '\n",
    "\n",
    "    ax1.plot(x, train_loss_list, 'b-', label=\"train_loss\", lw=1)\n",
    "    ax1.plot(x, test_loss_list, 'r-', label=\"test_loss\", lw=1)\n",
    "    ax1.set_title(info + 'Loss')\n",
    "    ax1.legend(loc='best', frameon=False)\n",
    "    ax1.set_xlabel(\"epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "    ax2.plot(x, train_acc_list, 'b-', label=\"train_accuracy\", lw=1)\n",
    "    ax2.plot(x, test_acc_list, 'r-', label=\"test_accuracy\", lw=1)\n",
    "    ax2.set_title(info + 'Accuracy')\n",
    "    ax2.legend(loc='best', frameon=False)\n",
    "    ax2.set_xlabel(\"epoch\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_by_dict(metrics: dict, shape: Tuple[int, int]):\n",
    "    \"\"\"\n",
    "    绘制图形 {指标: {参数值名1: [], 参数值名2: []}}\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(shape[0], shape[1])\n",
    "    axes = list(np.array(axes).ravel())\n",
    "    fig.set_size_inches(10*shape[0], 6*shape[1])\n",
    "\n",
    "    colors = ['b', 'r', 'y', 'c', 'k', 'g', 'w', 'm']\n",
    "    for ax_no, title in enumerate(metrics.keys()):\n",
    "        ax = axes[ax_no]\n",
    "        for i, (key, values) in enumerate(metrics.get(title, {}).items()):\n",
    "            x = np.linspace(0, len(values), len(values))\n",
    "            ax.plot(x, values, colors[i], label=key, lw=1)\n",
    "            ax.set_title(title)\n",
    "            ax.legend(loc='best', frameon=False)\n",
    "            ax.set_xlabel(\"epoch\")\n",
    "            ax.set_ylabel(title.lower().split(' ')[-1])\n",
    "\n",
    "    plt.show()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[HAMI-core Msg(17068:139786905402240:libvgpu.c:836)]: Initializing.....\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "banned-engineer",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 手动生成二分类任务的数据集\n",
    "+ 共生成两个数据集。\n",
    "+ 两个数据集的大小均为10000且训练集大小为7000，测试集大小为3000。\n",
    "+ 两个数据集的样本特征`x的维度均为200`，且`分别服从均值互为相反数且方差相同的正态分布`。\n",
    "+ 两个数据集的样本`标签分别为0和1`。"
   ]
  },
  {
   "cell_type": "code",
   "id": "veterinary-robertson",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-11-30T06:11:31.485643Z",
     "start_time": "2024-11-30T06:11:31.384045Z"
    }
   },
   "source": [
    "torch.random.manual_seed(1)\n",
    "\n",
    "p = 200\n",
    "mu = 5\n",
    "std = 0.1\n",
    "train_size = 7000\n",
    "test_size = 3000\n",
    "\n",
    "x_train = torch.vstack([\n",
    "    torch.normal(mean=mu, std=std, size=(train_size, p)),\n",
    "    torch.normal(mean=-mu, std=std, size=(train_size, p))\n",
    "])\n",
    "y_train = torch.hstack([\n",
    "    torch.ones(train_size, dtype=torch.float),\n",
    "    torch.zeros(train_size, dtype=torch.float),\n",
    "])\n",
    "x_test = torch.vstack([\n",
    "    torch.normal(mean=mu, std=std, size=(test_size, p)),\n",
    "    torch.normal(mean=-mu, std=std, size=(test_size, p))\n",
    "])\n",
    "y_test = torch.hstack([\n",
    "    torch.ones(test_size, dtype=torch.float),\n",
    "    torch.zeros(test_size, dtype=torch.float),\n",
    "])\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "# 构造数据集\n",
    "train_set = Data.TensorDataset(x_train, y_train)\n",
    "test_set = Data.TensorDataset(x_test, y_test)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14000, 200]) torch.Size([14000])\n",
      "torch.Size([6000, 200]) torch.Size([6000])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 手动实现前馈神经网络\n",
    "分析实验结果并绘制训练集和测试集的loss曲线"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "id": "aae03294686fcd8b"
  },
  {
   "cell_type": "code",
   "id": "appointed-sustainability",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-11-30T06:13:49.791923Z",
     "start_time": "2024-11-30T06:11:33.133878Z"
    }
   },
   "source": [
    "def data_iter(x: torch.Tensor, y: torch.Tensor, batch_size: int=8,\n",
    "              seed=1, shuffle=True):\n",
    "    \"\"\" 数据集生成器 \"\"\"\n",
    "\n",
    "    num_samples = x.shape[0]\n",
    "    indices = list(range(num_samples))\n",
    "    if shuffle:\n",
    "        random.seed(seed)\n",
    "        random.shuffle(indices)\n",
    "\n",
    "    # 构造小批次数据集\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "\n",
    "        # 选取的该批次内的行索引\n",
    "        j = torch.tensor(indices[i: min(i+batch_size, num_samples)])\n",
    "\n",
    "        yield x.index_select(dim=0, index=j), y.index_select(0, j)\n",
    "\n",
    "\n",
    "def sgd(lr, *params):\n",
    "    \"\"\"\n",
    "    优化器-梯度下降\n",
    "    :param lr:          学习率\n",
    "    :param params:      参数列表\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for param in params:\n",
    "        # 注意这里更改param时用的param.data\n",
    "        param.data -= lr * param.grad\n",
    "\n",
    "def sigmoid(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    激活函数-逻辑斯蒂回归\n",
    "    x: [batch_size, ]\n",
    "    :return [batch_size, 1]\n",
    "    \"\"\"\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "\n",
    "def binary_cross_entropy(y_hat: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    损失函数-二元交叉熵\n",
    "    :param y_hat(one_hot):   预测值 (batch_size, 1)\n",
    "    :param y_true:           真值  (batch_size)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    y_hat = y_hat.view(y_true.size())\n",
    "    l =  y_true * torch.log(y_hat) + (1-y_true) * torch.log(1 - y_hat)\n",
    "    return - l.sum()/y_true.shape[0]\n",
    "\n",
    "\n",
    "def relu(x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    线性单元\n",
    "    :param x:\n",
    "    :param gamma:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    x = torch.where(x >= 0, x, torch.zeros(x.size()))\n",
    "    return x\n",
    "\n",
    "def neural_net(x: torch.Tensor, *params) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    前馈神经网络\n",
    "    :param x:       特征\n",
    "    :param params:  模型参数\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    w1, b1, w2, b2 = params\n",
    "    # hidden = tanh(torch.mm(x, w1) + b1)\n",
    "    # hidden = leak_relu(torch.mm(x, w1) + b1)\n",
    "    hidden = relu(torch.matmul(x, w1) + b1)\n",
    "    return sigmoid(torch.matmul(hidden, w2) + b2)\n",
    "\n",
    "\n",
    "def evaluate_loss_acc(data_iter, net, loss_fn, *params):\n",
    "    \"\"\"\n",
    "    返回测试集的loss\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    test_l_sum, acc_sum, num_batch, n = 0.0, 0.0, 0, 0\n",
    "    for x, y_true in data_iter:\n",
    "\n",
    "        y_hat = net(x, *params)\n",
    "        y_hat: torch.Tensor = y_hat.view(y_true.size())\n",
    "\n",
    "        test_l_sum += loss_fn(y_hat, y_true)\n",
    "\n",
    "        y_hat = torch.where(y_hat > 0.5, 1, 0)\n",
    "        acc_sum += (y_hat == y_true).float().sum().item()\n",
    "\n",
    "        num_batch += 1\n",
    "        n += y_true.shape[0]\n",
    "\n",
    "    return test_l_sum/num_batch, acc_sum/n\n",
    "\n",
    "\n",
    "# 参数配置\n",
    "num_inputs = x_train.shape[1]\n",
    "num_hiddens = 128\n",
    "num_outputs = 1\n",
    "\n",
    "batch_size = 256\n",
    "num_epochs = 50\n",
    "lr = 0.1\n",
    "net = neural_net\n",
    "loss = binary_cross_entropy\n",
    "\n",
    "# 模型训练 w = [w_0, ..., w_n]\n",
    "w1 = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens)),\n",
    "                  dtype=torch.float32)\n",
    "b1 = torch.zeros(num_hiddens, dtype=torch.float32)\n",
    "w2 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_outputs)),\n",
    "                  dtype=torch.float32)\n",
    "b2 = torch.zeros(num_outputs, dtype=torch.float32)\n",
    "\n",
    "params = (w1, b1, w2, b2)\n",
    "for param in params:\n",
    "    param.requires_grad_(requires_grad=True)\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # 读取数据集\n",
    "    iter_train = data_iter(x_train, y_train, batch_size=batch_size)\n",
    "    iter_test = data_iter(x_test, y_test, batch_size=batch_size)\n",
    "\n",
    "    # 一批次的训练数据\n",
    "    train_l_sum, train_acc_sum, num_batch, n = 0.0, 0.0, 0, 0\n",
    "    for X, y_true in iter_train:\n",
    "\n",
    "        # 模型预测值\n",
    "        y_hat:torch.Tensor = net(X, *params)\n",
    "\n",
    "        # 损失值\n",
    "        l = loss(y_hat, y_true)\n",
    "\n",
    "        # 反向传播\n",
    "        l.backward()\n",
    "\n",
    "        # 随机梯度下降\n",
    "        sgd(lr, *params)\n",
    "\n",
    "        # 梯度置零\n",
    "        for param in params:\n",
    "            param.grad.data.zero_()\n",
    "\n",
    "        y_hat = y_hat.view(y_true.size())\n",
    "        y_hat = torch.where(y_hat > 0.5, 1, 0)\n",
    "        train_acc_sum += (y_hat == y_true).float().sum().item()\n",
    "        train_l_sum += l\n",
    "\n",
    "        num_batch += 1\n",
    "        n += y_true.shape[0]\n",
    "\n",
    "    train_l = train_l_sum / num_batch\n",
    "    train_acc = train_acc_sum / n\n",
    "    test_l, test_acc = evaluate_loss_acc(iter_test, net, loss, *params)\n",
    "\n",
    "    train_loss.append(train_l.detach().numpy())\n",
    "    test_loss.append(test_l.detach().numpy())\n",
    "    train_accuracy.append(train_acc)\n",
    "    test_accuracy.append(test_acc)\n",
    "    print('epoch %d, train_loss %f, test_loss %f, train_acc %f, test_acc %f'\n",
    "          % (epoch+1, train_l, test_l, train_acc_sum/n, test_acc))\n",
    "\n",
    "\n",
    "plot_loss_accuracy(train_loss, train_accuracy, test_loss, test_accuracy)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[HAMI-core Warn(17068:139786905402240:utils.c:183)]: get default cuda from (null)\n",
      "[HAMI-core Msg(17068:139786905402240:libvgpu.c:855)]: Initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train_loss 0.018377, test_loss 0.000361, train_acc 0.990857, test_acc 1.000000\n",
      "epoch 2, train_loss 0.000241, test_loss 0.000165, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 3, train_loss 0.000131, test_loss 0.000105, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 4, train_loss 0.000089, test_loss 0.000076, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 5, train_loss 0.000067, test_loss 0.000059, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 6, train_loss 0.000053, test_loss 0.000048, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 7, train_loss 0.000044, test_loss 0.000040, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 8, train_loss 0.000038, test_loss 0.000035, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 9, train_loss 0.000033, test_loss 0.000031, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 10, train_loss 0.000029, test_loss 0.000027, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 11, train_loss 0.000026, test_loss 0.000024, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 12, train_loss 0.000023, test_loss 0.000022, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 13, train_loss 0.000021, test_loss 0.000020, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 14, train_loss 0.000019, test_loss 0.000019, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 15, train_loss 0.000018, test_loss 0.000017, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 16, train_loss 0.000017, test_loss 0.000016, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 17, train_loss 0.000016, test_loss 0.000015, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 18, train_loss 0.000015, test_loss 0.000014, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 19, train_loss 0.000014, test_loss 0.000013, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 20, train_loss 0.000013, test_loss 0.000013, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 21, train_loss 0.000012, test_loss 0.000012, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 22, train_loss 0.000012, test_loss 0.000011, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 23, train_loss 0.000011, test_loss 0.000011, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 24, train_loss 0.000011, test_loss 0.000010, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 25, train_loss 0.000010, test_loss 0.000010, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 26, train_loss 0.000010, test_loss 0.000009, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 27, train_loss 0.000009, test_loss 0.000009, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 28, train_loss 0.000009, test_loss 0.000009, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 29, train_loss 0.000009, test_loss 0.000008, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 30, train_loss 0.000008, test_loss 0.000008, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 31, train_loss 0.000008, test_loss 0.000008, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 32, train_loss 0.000008, test_loss 0.000007, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 33, train_loss 0.000007, test_loss 0.000007, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 34, train_loss 0.000007, test_loss 0.000007, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 35, train_loss 0.000007, test_loss 0.000007, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 36, train_loss 0.000007, test_loss 0.000007, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 37, train_loss 0.000006, test_loss 0.000006, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 38, train_loss 0.000006, test_loss 0.000006, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 39, train_loss 0.000006, test_loss 0.000006, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 40, train_loss 0.000006, test_loss 0.000006, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 41, train_loss 0.000006, test_loss 0.000006, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 42, train_loss 0.000006, test_loss 0.000006, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 43, train_loss 0.000005, test_loss 0.000005, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 44, train_loss 0.000005, test_loss 0.000005, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 45, train_loss 0.000005, test_loss 0.000005, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 46, train_loss 0.000005, test_loss 0.000005, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 47, train_loss 0.000005, test_loss 0.000005, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 48, train_loss 0.000005, test_loss 0.000005, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 49, train_loss 0.000005, test_loss 0.000005, train_acc 1.000000, test_acc 1.000000\n",
      "epoch 50, train_loss 0.000005, test_loss 0.000005, train_acc 1.000000, test_acc 1.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### torch.nn实现前馈神经网络\n",
    "分析实验结果并绘制训练集和测试集的loss曲线"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "id": "c3134a418642d924"
  },
  {
   "cell_type": "code",
   "source": [
    "class TorchNeuron(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size,\n",
    "                 activation=nn.ReLU, num_layers=1, drop_out_rate=0):\n",
    "        super(TorchNeuron, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.activation = activation\n",
    "        self.drop_out_rate = drop_out_rate\n",
    "\n",
    "        self.linear = nn.Sequential(*self.create_linear_layers())\n",
    "\n",
    "        for param in self.linear.parameters():\n",
    "            nn.init.normal_(param, std=0.01)\n",
    "\n",
    "    def create_linear_layers(self) -> Tuple[nn.Module]:\n",
    "        \"\"\"\n",
    "        创建层\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        modules = [\n",
    "            nn.Linear(self.input_size, self.hidden_size),\n",
    "            self.activation(),\n",
    "            nn.Dropout(self.drop_out_rate),\n",
    "        ]\n",
    "        for i in range(self.num_layers - 1):\n",
    "            modules.append(nn.Linear(self.hidden_size, self.hidden_size))\n",
    "            modules.append(self.activation())\n",
    "            modules.append(nn.Dropout(self.drop_out_rate))\n",
    "        modules.append(nn.Linear(self.hidden_size, self.output_size))\n",
    "        return tuple(modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-11-30T06:13:49.800222Z",
     "start_time": "2024-11-30T06:13:49.793727Z"
    }
   },
   "id": "371a4582db4e21e6",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# 参数配置\n",
    "device = 'cpu'\n",
    "input_size = x_train.shape[1]\n",
    "hidden_size = 256\n",
    "output_size = 2\n",
    "\n",
    "batch_size = 256\n",
    "num_epochs = 50\n",
    "lr = 0.1\n",
    "\n",
    "net = TorchNeuron(input_size, hidden_size, output_size)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "# 构造数据集\n",
    "dataset_train = Data.TensorDataset(x_train, y_train)\n",
    "dataset_test = Data.TensorDataset(x_test, y_test)\n",
    "\n",
    "# 读取数据集\n",
    "iter_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "iter_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    test_data_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    train_loss, train_acc = train_epoch(data_loader=train_data_loader, net=net,\n",
    "                                        loss_func=loss_func, optimizer=optimizer, device=device)\n",
    "    test_loss, test_acc = test_epoch(data_loader=test_data_loader, net=net,\n",
    "                                     loss_func=loss_func, device=device)\n",
    "\n",
    "    print('epoch %d, train_loss %f, test_loss %f, train_acc %f, test_acc %f'\n",
    "          % (epoch+1, train_loss, test_loss, train_acc, test_acc))\n",
    "\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    test_loss_list.append(test_loss)\n",
    "    test_acc_list.append(test_acc)\n",
    "\n",
    "plot_loss_accuracy(train_loss_list, train_acc_list, test_loss_list, test_acc_list)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-11-30T06:13:50.467216Z",
     "start_time": "2024-11-30T06:13:49.801451Z"
    }
   },
   "id": "72e20699e67d879e",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 32\u001B[0m\n\u001B[1;32m     29\u001B[0m train_data_loader \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(train_set, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     30\u001B[0m test_data_loader \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(test_set, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 32\u001B[0m train_loss, train_acc \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_data_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnet\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[43m                                    \u001B[49m\u001B[43mloss_func\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m test_loss, test_acc \u001B[38;5;241m=\u001B[39m test_epoch(data_loader\u001B[38;5;241m=\u001B[39mtest_data_loader, net\u001B[38;5;241m=\u001B[39mnet,\n\u001B[1;32m     35\u001B[0m                                  loss_func\u001B[38;5;241m=\u001B[39mloss_func, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepoch \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m, train_loss \u001B[39m\u001B[38;5;132;01m%f\u001B[39;00m\u001B[38;5;124m, test_loss \u001B[39m\u001B[38;5;132;01m%f\u001B[39;00m\u001B[38;5;124m, train_acc \u001B[39m\u001B[38;5;132;01m%f\u001B[39;00m\u001B[38;5;124m, test_acc \u001B[39m\u001B[38;5;132;01m%f\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     38\u001B[0m       \u001B[38;5;241m%\u001B[39m (epoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m, train_loss, test_loss, train_acc, test_acc))\n",
      "Cell \u001B[0;32mIn[1], line 38\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[0;34m(data_loader, net, loss_func, optimizer, device)\u001B[0m\n\u001B[1;32m     35\u001B[0m y_true \u001B[38;5;241m=\u001B[39m y_true\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mlong()\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# 计算损失\u001B[39;00m\n\u001B[0;32m---> 38\u001B[0m y_hat: torch\u001B[38;5;241m.\u001B[39mTensor \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_func(y_hat, y_true)\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# 取概率最大的类别索引\u001B[39;00m\n",
      "File \u001B[0;32m/app/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/app/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[5], line 35\u001B[0m, in \u001B[0;36mTorchNeuron.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 35\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/app/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/app/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/app/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 219\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    220\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m/app/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/app/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/app/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 采用10折交叉验证评估实验结果\n",
    "要求除了最终结果外还需以表格的形式展示每折的实验结果"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "id": "439ca54d65a4b99d"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchNeuron(\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=200, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "lr=0.1 epoch 1, train_loss 0.009769, test_loss 0.000130, train_acc 0.994444, test_acc 1.000000\n",
      "lr=0.1 epoch 2, train_loss 0.000102, test_loss 0.000062, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 3, train_loss 0.000066, test_loss 0.000041, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 4, train_loss 0.000052, test_loss 0.000031, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 5, train_loss 0.000042, test_loss 0.000025, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 6, train_loss 0.000036, test_loss 0.000022, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 7, train_loss 0.000033, test_loss 0.000020, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 8, train_loss 0.000031, test_loss 0.000018, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 9, train_loss 0.000029, test_loss 0.000016, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 10, train_loss 0.000027, test_loss 0.000016, train_acc 1.000000, test_acc 1.000000\n",
      "TorchNeuron(\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=200, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "lr=0.1 epoch 1, train_loss 0.010057, test_loss 0.000093, train_acc 0.993500, test_acc 1.000000\n",
      "lr=0.1 epoch 2, train_loss 0.000104, test_loss 0.000047, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 3, train_loss 0.000065, test_loss 0.000032, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 4, train_loss 0.000050, test_loss 0.000025, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 5, train_loss 0.000042, test_loss 0.000021, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 6, train_loss 0.000036, test_loss 0.000018, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 7, train_loss 0.000032, test_loss 0.000016, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 8, train_loss 0.000029, test_loss 0.000015, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 9, train_loss 0.000027, test_loss 0.000014, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 10, train_loss 0.000026, test_loss 0.000013, train_acc 1.000000, test_acc 1.000000\n",
      "TorchNeuron(\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=200, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "lr=0.1 epoch 1, train_loss 0.010436, test_loss 0.000094, train_acc 0.992000, test_acc 1.000000\n",
      "lr=0.1 epoch 2, train_loss 0.000107, test_loss 0.000048, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 3, train_loss 0.000067, test_loss 0.000033, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 4, train_loss 0.000051, test_loss 0.000026, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 5, train_loss 0.000042, test_loss 0.000021, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 6, train_loss 0.000036, test_loss 0.000019, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 7, train_loss 0.000032, test_loss 0.000017, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 8, train_loss 0.000029, test_loss 0.000016, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 9, train_loss 0.000027, test_loss 0.000015, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 10, train_loss 0.000026, test_loss 0.000014, train_acc 1.000000, test_acc 1.000000\n",
      "TorchNeuron(\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=200, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "lr=0.1 epoch 1, train_loss 0.009859, test_loss 0.000094, train_acc 0.995556, test_acc 1.000000\n",
      "lr=0.1 epoch 2, train_loss 0.000108, test_loss 0.000048, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 3, train_loss 0.000069, test_loss 0.000033, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 4, train_loss 0.000050, test_loss 0.000026, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 5, train_loss 0.000042, test_loss 0.000021, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 6, train_loss 0.000036, test_loss 0.000019, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 7, train_loss 0.000032, test_loss 0.000017, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 8, train_loss 0.000030, test_loss 0.000016, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 9, train_loss 0.000028, test_loss 0.000015, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 10, train_loss 0.000027, test_loss 0.000014, train_acc 1.000000, test_acc 1.000000\n",
      "TorchNeuron(\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=200, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "lr=0.1 epoch 1, train_loss 0.009548, test_loss 0.000126, train_acc 0.996944, test_acc 1.000000\n",
      "lr=0.1 epoch 2, train_loss 0.000093, test_loss 0.000062, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 3, train_loss 0.000062, test_loss 0.000042, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 4, train_loss 0.000049, test_loss 0.000032, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 5, train_loss 0.000041, test_loss 0.000027, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 6, train_loss 0.000035, test_loss 0.000023, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 7, train_loss 0.000033, test_loss 0.000021, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 8, train_loss 0.000029, test_loss 0.000019, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 9, train_loss 0.000027, test_loss 0.000018, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 10, train_loss 0.000027, test_loss 0.000017, train_acc 1.000000, test_acc 1.000000\n",
      "TorchNeuron(\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=200, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "lr=0.1 epoch 1, train_loss 0.009969, test_loss 0.000116, train_acc 0.994611, test_acc 1.000000\n",
      "lr=0.1 epoch 2, train_loss 0.000094, test_loss 0.000058, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 3, train_loss 0.000062, test_loss 0.000040, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 4, train_loss 0.000046, test_loss 0.000031, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 5, train_loss 0.000039, test_loss 0.000026, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 6, train_loss 0.000034, test_loss 0.000023, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 7, train_loss 0.000030, test_loss 0.000020, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 8, train_loss 0.000029, test_loss 0.000019, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 9, train_loss 0.000027, test_loss 0.000018, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 10, train_loss 0.000025, test_loss 0.000017, train_acc 1.000000, test_acc 1.000000\n",
      "TorchNeuron(\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=200, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "lr=0.1 epoch 1, train_loss 0.009845, test_loss 0.000122, train_acc 0.995667, test_acc 1.000000\n",
      "lr=0.1 epoch 2, train_loss 0.000116, test_loss 0.000062, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 3, train_loss 0.000072, test_loss 0.000043, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 4, train_loss 0.000055, test_loss 0.000033, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 5, train_loss 0.000044, test_loss 0.000028, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 6, train_loss 0.000040, test_loss 0.000024, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 7, train_loss 0.000034, test_loss 0.000022, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 8, train_loss 0.000031, test_loss 0.000020, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 9, train_loss 0.000029, test_loss 0.000019, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 10, train_loss 0.000028, test_loss 0.000018, train_acc 1.000000, test_acc 1.000000\n",
      "TorchNeuron(\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=200, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "lr=0.1 epoch 1, train_loss 0.010350, test_loss 0.000111, train_acc 0.993278, test_acc 1.000000\n",
      "lr=0.1 epoch 2, train_loss 0.000111, test_loss 0.000055, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 3, train_loss 0.000070, test_loss 0.000037, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 4, train_loss 0.000053, test_loss 0.000028, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 5, train_loss 0.000041, test_loss 0.000024, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 6, train_loss 0.000037, test_loss 0.000021, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 7, train_loss 0.000033, test_loss 0.000019, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 8, train_loss 0.000031, test_loss 0.000017, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 9, train_loss 0.000028, test_loss 0.000016, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 10, train_loss 0.000027, test_loss 0.000015, train_acc 1.000000, test_acc 1.000000\n",
      "TorchNeuron(\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=200, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "lr=0.1 epoch 1, train_loss 0.009715, test_loss 0.000077, train_acc 0.997278, test_acc 1.000000\n",
      "lr=0.1 epoch 2, train_loss 0.000104, test_loss 0.000040, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 3, train_loss 0.000067, test_loss 0.000028, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 4, train_loss 0.000052, test_loss 0.000022, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 5, train_loss 0.000043, test_loss 0.000019, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 6, train_loss 0.000038, test_loss 0.000016, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 7, train_loss 0.000033, test_loss 0.000015, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 8, train_loss 0.000031, test_loss 0.000014, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 9, train_loss 0.000029, test_loss 0.000013, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 10, train_loss 0.000027, test_loss 0.000012, train_acc 1.000000, test_acc 1.000000\n",
      "TorchNeuron(\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=200, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "lr=0.1 epoch 1, train_loss 0.009902, test_loss 0.000104, train_acc 0.995278, test_acc 1.000000\n",
      "lr=0.1 epoch 2, train_loss 0.000112, test_loss 0.000052, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 3, train_loss 0.000070, test_loss 0.000035, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 4, train_loss 0.000052, test_loss 0.000027, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 5, train_loss 0.000044, test_loss 0.000023, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 6, train_loss 0.000038, test_loss 0.000020, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 7, train_loss 0.000034, test_loss 0.000018, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 8, train_loss 0.000031, test_loss 0.000016, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 9, train_loss 0.000029, test_loss 0.000015, train_acc 1.000000, test_acc 1.000000\n",
      "lr=0.1 epoch 10, train_loss 0.000027, test_loss 0.000014, train_acc 1.000000, test_acc 1.000000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def train(train_data_loader, test_data_loader):\n",
    "\n",
    "    net = TorchNeuron(input_size, hidden_size, output_size, drop_out_rate=0.3)\n",
    "    print(net)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=0.0)\n",
    "\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    test_loss_list = []\n",
    "    test_acc_list = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_loss, train_acc = train_epoch(data_loader=train_data_loader, net=net,\n",
    "                                            loss_func=loss_func, optimizer=optimizer, device=device)\n",
    "        test_loss, test_acc = test_epoch(data_loader=test_data_loader, net=net,\n",
    "                                         loss_func=loss_func, device=device)\n",
    "\n",
    "        print('lr=%s epoch %d, train_loss %f, test_loss %f, train_acc %f, test_acc %f'\n",
    "              % (lr, epoch+1, train_loss, test_loss, train_acc, test_acc))\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_loss_list.append(test_loss)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "    return train_loss_list, train_acc_list, test_loss_list, test_acc_list\n",
    "\n",
    "# 参数配置\n",
    "device = 'cpu'\n",
    "input_size = x_train.shape[1]\n",
    "hidden_size = 256\n",
    "output_size = 2\n",
    "\n",
    "batch_size = 256\n",
    "num_epochs = 10\n",
    "lr = 0.1\n",
    "\n",
    "X = torch.cat((x_train, x_test))\n",
    "y = torch.cat((y_train, y_test))\n",
    "\n",
    "indices = np.arange(0, X.shape[0])\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "kf_train_loss, kf_train_acc, kf_test_loss, kf_test_acc = [], [], [], []\n",
    "for train_index, test_index in kf.split(indices):\n",
    "    train_index = torch.LongTensor(train_index)\n",
    "    test_index = torch.LongTensor(test_index)\n",
    "\n",
    "    train_x = torch.index_select(X, dim=0, index=train_index)\n",
    "    train_y = torch.index_select(y, dim=0, index=train_index)\n",
    "    test_x = torch.index_select(X, dim=0, index=test_index)\n",
    "    test_y = torch.index_select(y, dim=0, index=test_index)\n",
    "\n",
    "    train_set = Data.TensorDataset(train_x, train_y)\n",
    "    test_set = Data.TensorDataset(test_x, test_y)\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    test_data_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 训练\n",
    "    train_loss_list, train_acc_list, test_loss_list, test_acc_list = train(train_data_loader, test_data_loader)\n",
    "\n",
    "    kf_train_loss.append(train_loss_list)\n",
    "    kf_train_acc.append(train_acc_list)\n",
    "    kf_test_loss.append(test_loss_list)\n",
    "    kf_test_acc.append(test_acc_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "745a2536f859592a"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.55238345e-05 1.34269281e-05 1.41567948e-05 1.38647451e-05\n",
      " 1.70274982e-05 1.67884380e-05 1.78620262e-05 1.52344270e-05\n",
      " 1.20956392e-05 1.44826975e-05]\n",
      "[3.80892912e-05 2.95931347e-05 3.02551834e-05 3.00995494e-05\n",
      " 3.87915284e-05 3.66381004e-05 3.89902085e-05 3.42971369e-05\n",
      " 2.55111829e-05 3.24274791e-05]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "kf_train_loss_np = np.array(kf_train_loss)\n",
    "kf_train_acc_np = np.array(kf_train_acc)\n",
    "\n",
    "kf_test_loss_np = np.array(kf_test_loss)\n",
    "kf_test_acc_np = np.array(kf_test_acc)\n",
    "\n",
    "print(kf_test_loss_np.min(axis=1))\n",
    "print(kf_test_loss_np.mean(axis=1))\n",
    "\n",
    "print(kf_test_acc_np.max(axis=1))\n",
    "print(kf_test_acc_np.mean(axis=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "id": "8e6a9e2be813be62"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
