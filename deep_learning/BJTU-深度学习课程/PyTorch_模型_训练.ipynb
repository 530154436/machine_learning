{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cheap-breakdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-conversion",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 线性回归实现\n",
    "线性回归(Linear Regression)是机器学习和统计学中最基础和广泛应用的模型，是一种对自变量和因变量之间关系进行建模的回归分析。\n",
    "从机器学习的角度来看，自变量就是样本的特征向量 $x \\in R$ (每一维对应一个自变量)，因变量是标签 $y$，这里 $x \\in R$ 是连续值。假设空间是一组参数化的线性函数\n",
    "$$\n",
    "f(x;𝑤,𝑏) = 𝑥w^T+𝑏\n",
    "$$\n",
    "其中，权重向量 $𝒘$ 和偏置 $𝒃$ 是线性回归需要学习的参数， 函数 $f(x;𝑤,𝑏)$ 称为线性模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-embassy",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 手动实现线性回归\n",
    "##### 生成数据\n",
    "设训练数据集样本数为1000，输入个数(特征数)为2，给定随机生成的批量样本特征 $X \\in R^{1000 \\times 2}$。 <br>\n",
    "线性回归模型的真实权重 $ w = [2 , −3.4]^T $ 和偏差 $𝑏 = 4.2$, 以及一个随机噪声项 $\\epsilon$ 来生成标签。\n",
    "$$𝑦 = 𝑥𝑤^T + 𝑏 + \\epsilon$$\n",
    "其中，噪声项 $\\epsilon$ 服从均值0、标准差为0.01的`正态分布`。噪声代表了数据中无意义的干扰。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "palestinian-mixer",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.7951, -0.4721],\n",
       "         [ 0.4241, -0.1669],\n",
       "         [ 0.0801,  0.7826],\n",
       "         ...,\n",
       "         [ 2.7256,  0.2159],\n",
       "         [ 2.2605, -0.3228],\n",
       "         [-0.4128,  0.4221]]),\n",
       " tensor([ 7.3959e+00,  5.6073e+00,  1.6924e+00,  3.4739e+00,  1.0654e+00,\n",
       "          4.5220e+00,  4.1421e+00,  9.8292e+00,  5.0117e+00,  5.6048e+00,\n",
       "          1.9620e+00,  6.0398e+00, -3.7034e+00,  1.5667e+00,  5.4652e+00,\n",
       "          1.1373e+00,  7.7659e+00,  8.4604e+00,  3.3348e+00,  9.1011e+00,\n",
       "          5.6248e+00,  3.4115e+00,  6.5809e+00,  2.4064e+00, -8.5358e-01,\n",
       "          1.2493e+00,  2.7074e+00,  4.4445e+00,  9.6267e+00,  1.5189e+01,\n",
       "          7.3295e+00,  5.1481e+00,  4.0859e+00,  1.4094e+00,  4.3402e+00,\n",
       "          4.0373e+00,  3.0124e+00,  5.6115e+00, -3.1837e+00, -1.0741e+00,\n",
       "          2.5680e+00, -5.8867e+00,  4.1949e+00,  9.1337e+00,  9.8848e+00,\n",
       "         -6.8707e-01,  8.1947e+00,  3.9214e-01,  1.1323e+00,  6.0221e+00,\n",
       "          1.8696e+00,  9.8574e+00,  6.8252e+00,  4.3746e+00,  4.0301e+00,\n",
       "          1.0520e+01,  4.8938e+00,  7.9282e+00,  5.6263e+00,  9.6226e-01,\n",
       "          9.2753e+00,  1.0482e+01,  4.9068e+00,  9.6731e+00,  6.0911e+00,\n",
       "          9.1995e+00,  4.3032e+00,  3.2295e+00,  7.7238e+00,  1.3555e+01,\n",
       "          3.2141e+00,  6.0031e+00, -3.8408e+00,  8.3323e+00, -3.3200e-01,\n",
       "          6.2480e+00,  1.1199e+01,  3.9246e+00,  1.4785e+01,  9.4972e+00,\n",
       "          2.8899e+00,  3.9900e+00,  9.0215e+00,  5.6765e+00,  1.5594e+00,\n",
       "          1.0025e+01,  3.3196e+00,  3.9293e+00,  1.3682e+00,  8.9147e+00,\n",
       "          9.6823e+00,  2.1950e+00,  3.2848e+00,  4.3609e+00,  2.3878e+00,\n",
       "          2.1398e+00,  1.5335e+00,  4.8626e+00,  7.6661e+00,  8.0429e+00,\n",
       "          1.9135e-01,  2.9941e-01,  8.2009e+00, -2.5212e+00,  6.7779e+00,\n",
       "          4.2729e-01,  6.5405e+00,  1.5105e+00,  5.1671e+00,  6.5978e+00,\n",
       "          4.2124e+00,  4.8386e+00,  1.2175e+01,  6.6750e+00,  4.3275e+00,\n",
       "          6.9235e+00,  6.5492e+00,  9.3437e+00,  2.2558e+00,  7.6045e-01,\n",
       "          5.4700e+00,  7.9796e+00,  5.3470e-01,  5.1393e+00,  8.7102e+00,\n",
       "          4.5631e+00,  8.3555e+00,  4.4052e+00, -1.3066e+00,  6.3622e+00,\n",
       "          2.6329e+00,  7.1254e+00,  5.4971e+00,  1.3860e+01,  5.0330e+00,\n",
       "          5.5700e+00,  4.5140e+00,  9.1410e+00,  7.9973e+00, -1.0453e+00,\n",
       "          3.0426e+00, -3.5810e+00, -9.5939e-01,  3.3162e+00,  9.2892e+00,\n",
       "          4.3207e+00,  3.6130e+00,  1.1146e-01,  4.4348e+00,  6.6835e+00,\n",
       "         -5.6173e+00,  1.0439e+00,  7.3411e+00,  2.2008e+00,  3.4817e+00,\n",
       "          1.0815e+01,  1.1785e+00,  2.1341e+00,  1.1458e+00,  6.0571e+00,\n",
       "          1.8347e+00,  4.6384e+00,  9.6491e+00,  2.2526e+00,  3.0168e+00,\n",
       "          8.5408e+00,  1.0210e+00,  7.6443e-01,  6.1524e+00,  2.3694e+00,\n",
       "          9.6939e+00,  4.0897e+00,  1.1495e+00,  1.2640e+01, -1.5352e+00,\n",
       "          1.6323e-01,  3.7217e+00,  7.4981e+00,  5.8381e+00, -5.8252e-01,\n",
       "          8.9154e+00,  1.1336e+01,  6.7114e+00,  3.9618e+00,  2.8721e+00,\n",
       "          1.4665e+00,  1.9770e+00,  7.0910e+00,  1.3472e+01,  2.3379e+00,\n",
       "          2.3841e+00,  2.1808e+00,  5.3636e+00,  6.6225e+00,  8.2669e+00,\n",
       "          7.5398e+00,  2.9711e+00, -1.0089e-01,  7.5652e+00,  8.6236e+00,\n",
       "         -1.0319e+00,  6.9967e+00,  5.6658e+00,  4.8975e+00,  2.6476e+00,\n",
       "          5.3752e+00,  3.4970e-01,  6.0321e+00, -1.1292e-01,  7.4404e+00,\n",
       "         -5.6949e+00,  1.3832e+01,  2.8132e+00,  4.2963e+00,  2.0222e+00,\n",
       "          4.1609e+00,  8.4999e+00,  6.4863e+00,  1.3692e+00, -4.8773e+00,\n",
       "          8.5887e+00,  1.0763e+01,  3.8941e+00,  2.5590e+00,  8.3360e+00,\n",
       "          8.8957e+00,  9.5939e+00,  3.7821e+00,  6.0741e-01,  9.6302e+00,\n",
       "         -2.6192e+00,  8.9756e+00,  4.1382e+00,  5.8678e+00,  6.7568e+00,\n",
       "          3.3071e+00, -5.3453e-01,  3.2658e+00,  2.3863e+00,  3.4435e+00,\n",
       "          6.8436e+00,  5.8572e+00,  9.4259e+00,  1.7897e-01, -1.2649e+00,\n",
       "         -1.5024e+00, -2.7717e+00,  7.8119e+00,  5.4943e+00, -2.7935e+00,\n",
       "          7.6597e+00,  9.0156e+00,  9.6173e-01,  2.2315e+00,  7.3167e+00,\n",
       "          4.3503e+00,  3.3368e+00,  2.3464e+00,  4.0901e+00,  3.1149e+00,\n",
       "          1.2450e+01,  5.7663e+00,  3.3116e+00, -3.0340e+00, -9.1096e+00,\n",
       "          7.1694e+00,  3.8531e+00,  6.0874e+00,  8.4748e+00,  6.5198e+00,\n",
       "          4.8391e+00,  1.4823e+01,  1.3396e+00,  9.8504e+00,  1.3570e+00,\n",
       "         -2.1337e+00,  5.5450e+00,  3.0737e+00,  8.1686e+00,  6.9748e-01,\n",
       "          1.8900e+00,  4.5348e+00,  8.9917e+00,  7.4374e-01,  7.9266e+00,\n",
       "         -1.1851e+00,  3.7549e+00,  1.0299e+01,  7.6860e+00,  2.0732e+00,\n",
       "          4.5404e+00,  3.9532e+00,  2.8082e+00,  4.5431e+00,  2.8607e+00,\n",
       "         -7.7514e-01,  1.2700e+01,  7.1608e-01,  1.8918e+00,  1.2808e+01,\n",
       "          4.8880e+00,  8.8899e+00,  1.2587e+00,  6.0810e+00,  1.1896e+01,\n",
       "          1.6795e+00,  1.8564e+00,  8.3467e+00,  8.2500e+00,  2.0085e+00,\n",
       "          1.0577e+01,  8.0493e+00,  3.7853e+00,  5.9991e+00,  4.2450e+00,\n",
       "          1.2661e+01,  2.2684e+00, -5.9566e+00,  3.0372e+00,  4.8551e+00,\n",
       "          6.1653e+00,  3.0337e+00,  4.1295e+00,  9.8481e+00,  6.7525e+00,\n",
       "          4.5801e+00,  8.0712e-02, -1.0103e+00,  1.7282e+00,  3.1462e+00,\n",
       "         -1.9210e+00,  1.0858e+01,  1.7084e+00,  7.9144e+00,  5.1472e+00,\n",
       "          5.6727e+00,  6.7378e+00,  6.5120e+00,  1.3555e+00, -5.5953e+00,\n",
       "         -5.5660e-01,  4.2884e+00,  5.1496e+00,  4.1045e+00,  4.9132e+00,\n",
       "          1.0902e+01,  5.6017e+00, -2.9590e+00, -2.6281e+00,  6.8372e+00,\n",
       "          6.0524e+00,  5.9947e+00,  5.0773e+00,  6.2656e+00,  1.2890e+01,\n",
       "          2.5520e+00,  4.6991e+00,  3.1228e+00, -3.8456e+00, -3.5835e+00,\n",
       "          8.4459e+00,  1.0482e+01,  1.1537e+00, -5.1274e+00,  1.3763e+01,\n",
       "          8.7048e+00,  7.7423e+00,  5.0893e+00, -1.1018e+00,  8.0011e+00,\n",
       "          3.7866e+00,  5.4419e+00,  3.1491e+00,  1.1408e+01,  1.6258e+00,\n",
       "          7.5191e+00,  1.8963e+00,  4.3008e+00,  2.6503e+00, -3.6233e+00,\n",
       "         -4.3591e-02, -3.1130e+00,  8.4539e+00,  7.8390e+00,  4.7508e+00,\n",
       "          1.1253e+00,  5.1083e+00,  8.5259e+00,  7.1549e+00, -4.6008e+00,\n",
       "          1.1588e+01,  4.9476e+00, -3.1574e-01,  6.0153e+00,  2.4298e+00,\n",
       "          8.4779e+00,  9.7320e+00, -2.0025e+00,  5.9070e+00,  3.4525e+00,\n",
       "          9.4943e+00, -2.0611e+00,  7.2458e+00,  1.5043e+01,  8.4938e+00,\n",
       "          1.0222e+01,  3.7804e+00,  4.1485e+00,  7.8595e+00,  3.0842e+00,\n",
       "          8.2063e+00, -6.0700e+00,  8.9104e-01,  2.4961e+00,  5.5469e+00,\n",
       "          3.4663e+00,  7.0623e+00,  8.7955e+00,  4.9596e+00,  6.9615e+00,\n",
       "          6.9591e+00,  6.4795e+00,  1.2247e+01, -1.0987e+00,  2.5928e+00,\n",
       "          6.6603e+00,  1.0497e+01,  5.1707e+00,  1.2062e+01,  9.1814e+00,\n",
       "          1.0520e+01,  6.6204e+00,  4.5834e+00, -9.4451e-01,  1.3508e+01,\n",
       "          6.7551e+00,  5.8719e+00,  3.8684e+00,  1.1948e+01,  6.3220e+00,\n",
       "          1.4431e+00,  3.5982e+00,  2.8530e+00,  4.8315e+00, -1.0131e+00,\n",
       "          9.6456e+00,  6.8778e+00, -2.7681e+00,  6.5674e+00,  3.7528e+00,\n",
       "          4.1468e+00,  3.1200e+00,  5.2421e+00, -1.9471e+00,  9.1704e+00,\n",
       "          6.9914e+00,  8.4108e+00, -5.6465e+00,  7.7010e+00,  8.4811e+00,\n",
       "          4.2856e+00,  1.3818e+00,  7.5346e+00,  6.6786e+00, -1.5865e+00,\n",
       "          3.5820e+00,  7.7620e+00,  5.5102e+00,  2.8655e-01,  2.4266e+00,\n",
       "          5.4071e+00,  3.7414e+00,  1.3692e+01, -1.1479e+00,  9.9752e-01,\n",
       "         -3.8508e+00,  1.5739e+00,  1.1270e+01,  1.2833e+01,  4.2467e+00,\n",
       "          4.6578e+00,  9.8896e-01,  4.0767e+00,  5.6668e+00,  4.7463e+00,\n",
       "          1.1323e+01,  3.3086e+00,  8.7361e+00,  2.0494e+00,  7.0054e+00,\n",
       "          4.2233e+00,  8.5645e+00, -3.7326e+00,  9.8306e+00,  7.0606e+00,\n",
       "         -1.9998e+00, -6.1697e-01,  2.2713e+00,  4.1781e+00,  7.8539e+00,\n",
       "          1.3914e+01,  2.1244e+00,  1.0569e+01,  9.3822e+00,  4.0455e-01,\n",
       "          4.2240e+00,  3.5368e+00,  8.9650e+00,  1.9574e+00,  9.5115e+00,\n",
       "          4.8213e+00,  1.0219e+01,  6.6381e+00,  8.2185e+00,  1.8971e+00,\n",
       "         -3.1543e+00,  8.9024e+00,  7.0985e+00,  2.3789e+00,  1.7913e+00,\n",
       "          1.3266e+00,  5.6701e+00,  2.7691e+00,  2.3220e+00,  5.5011e+00,\n",
       "          1.3296e+00,  5.8584e+00,  2.7524e+00,  1.2168e+01,  1.5417e+00,\n",
       "          2.5180e+00,  5.0381e+00,  4.2130e+00,  5.5992e+00,  5.8580e+00,\n",
       "          4.1382e+00,  5.2994e+00,  7.7033e+00,  7.5606e+00,  4.7013e+00,\n",
       "          1.1228e+00,  8.5046e+00,  2.7162e-01,  2.6652e+00, -2.2142e+00,\n",
       "         -1.2035e+00, -2.3776e+00,  4.4761e+00,  7.3840e+00,  3.0159e+00,\n",
       "         -2.1925e+00,  4.7387e+00,  8.5189e+00,  4.6174e+00,  8.4203e+00,\n",
       "          3.8914e+00,  2.4275e+00,  6.4645e+00,  3.4535e+00,  3.5415e+00,\n",
       "          1.8551e+00, -2.4435e+00, -2.8533e+00,  4.4401e+00,  1.5789e+00,\n",
       "          1.7492e+00,  5.3537e+00,  5.1027e+00,  2.5818e-01, -3.5548e+00,\n",
       "          5.4721e+00,  2.3190e+00,  4.6954e+00,  1.0658e+01,  8.0649e+00,\n",
       "          1.4184e+00,  6.6423e+00,  4.1318e+00,  5.9626e+00,  3.3163e+00,\n",
       "          8.0936e+00,  2.2912e+00,  5.7578e+00,  3.0818e+00,  6.4044e+00,\n",
       "          1.1169e+01,  6.3553e+00,  6.4173e+00, -2.6177e+00, -1.6059e+00,\n",
       "          1.1616e+00,  1.8703e+00,  2.9365e+00,  8.9877e+00,  5.1496e+00,\n",
       "          6.7431e+00,  8.6362e+00,  3.5079e+00,  7.1878e+00, -2.9463e+00,\n",
       "          1.1386e+01,  7.3912e+00, -5.7578e+00,  5.0149e+00,  5.6853e+00,\n",
       "          1.0465e+01,  5.0540e+00,  5.6285e+00,  4.9157e+00, -5.0815e-01,\n",
       "          2.3999e+00,  1.6977e+00, -1.2710e+00,  4.5859e+00,  3.1710e+00,\n",
       "         -2.3400e+00,  7.2905e+00,  2.2623e+00, -1.0159e+00,  5.2927e+00,\n",
       "          4.8859e+00,  8.6091e+00,  5.3466e+00,  3.3292e+00,  1.4164e+00,\n",
       "          1.9171e-01,  3.5661e+00,  8.5071e+00,  6.0112e+00, -3.2145e+00,\n",
       "          1.3187e+00,  1.8180e+00,  9.9452e+00, -1.3938e+00,  6.4886e+00,\n",
       "          9.7453e+00,  9.9839e+00,  5.9488e+00, -7.3422e-01,  3.3928e+00,\n",
       "          7.9403e+00,  8.6201e+00,  2.7144e+00, -9.4114e-01,  5.2109e+00,\n",
       "          7.2264e+00, -2.0190e+00, -2.4454e+00,  5.4950e+00,  9.5095e+00,\n",
       "          4.5066e+00,  1.0638e+01,  2.1087e+00,  3.0169e+00,  6.2730e-01,\n",
       "          2.7286e+00,  3.9755e+00,  1.0876e+01,  8.0977e+00,  4.6158e+00,\n",
       "         -1.3033e+00,  1.0556e+00,  3.6063e+00, -9.5149e-01,  6.5252e+00,\n",
       "          3.9095e+00,  7.0587e+00,  2.4514e+00,  3.8415e+00,  2.8112e+00,\n",
       "         -1.0774e-01, -2.2887e+00,  2.6822e-01,  3.5305e+00, -2.7177e+00,\n",
       "         -4.2543e+00,  3.2253e+00,  5.0705e+00,  3.5372e+00,  5.7169e+00,\n",
       "         -2.1565e+00,  1.2254e+00,  4.8967e+00,  1.7821e+00, -3.2055e+00,\n",
       "          4.4439e+00,  7.8813e+00,  4.6869e+00,  1.2141e+01,  2.4722e+00,\n",
       "          4.5791e+00, -5.1873e-01,  1.6644e+01,  7.6909e+00,  3.3682e+00,\n",
       "          4.0637e+00,  8.1743e+00,  6.5729e+00,  1.1950e+00,  1.1390e+01,\n",
       "          1.1061e+01,  6.9855e+00,  1.8269e+00,  4.9025e+00,  7.0400e-01,\n",
       "          4.9065e+00,  6.9944e-02,  4.3673e+00,  8.5607e+00,  7.1828e+00,\n",
       "          8.4487e+00,  2.6342e+00,  1.3417e+00,  3.8172e+00,  1.2607e+00,\n",
       "          2.2334e-01, -2.0946e+00,  3.3243e+00,  5.6077e+00,  1.2753e+01,\n",
       "         -3.3100e+00,  2.1990e+00,  4.1747e+00,  3.5884e+00,  8.0063e-01,\n",
       "         -6.4615e-01,  5.6981e+00,  5.6711e+00,  1.2260e+01,  6.3328e+00,\n",
       "          8.4199e+00,  1.0544e+01,  1.9976e-01,  1.3784e+00,  2.2565e+00,\n",
       "          4.7542e+00,  2.2580e+00,  5.4336e+00,  3.1957e+00,  6.0084e+00,\n",
       "          5.0782e+00,  4.9912e+00,  6.6219e+00, -2.6618e+00, -1.6473e+00,\n",
       "          4.6528e-01,  5.0096e-01,  5.2240e+00, -1.0049e+00,  3.9616e+00,\n",
       "          3.4933e+00,  4.4057e+00,  2.3569e-01,  4.2061e+00,  8.2126e-01,\n",
       "         -1.1421e+00,  1.0854e+00,  9.8863e+00,  3.1946e+00,  2.4151e+00,\n",
       "         -3.8705e+00,  1.7181e-01,  8.2788e+00,  6.9080e+00,  1.2649e+01,\n",
       "          6.8480e+00,  9.0835e-01,  5.0100e+00,  6.1309e+00,  1.7579e+00,\n",
       "         -1.7970e+00,  6.1836e+00, -6.7436e-01,  6.6454e+00,  8.5252e+00,\n",
       "          6.1682e+00,  1.2288e+00,  8.0895e+00,  9.8502e+00,  3.5794e+00,\n",
       "          4.0402e+00,  4.7444e+00,  2.4991e+00,  3.5973e+00,  3.8020e-01,\n",
       "          5.4715e+00,  4.7157e+00,  6.8980e+00,  7.1377e+00, -2.6033e+00,\n",
       "          3.1634e+00, -1.2537e+00, -1.9293e+00,  1.6218e+00,  3.3839e+00,\n",
       "          1.8179e-01,  4.1043e-01, -5.5684e-01, -2.6085e+00,  9.2996e+00,\n",
       "          6.5546e+00,  3.9424e+00,  1.2937e+01,  4.8472e+00,  8.5123e+00,\n",
       "         -4.6801e-01,  2.0171e+00,  1.1690e+00,  5.4038e+00, -3.5980e+00,\n",
       "          4.7954e+00, -2.3481e+00, -8.5551e+00,  1.0766e+01, -1.4156e-01,\n",
       "          2.8395e+00,  5.9780e+00, -6.6166e-01,  8.6435e+00, -1.3877e-01,\n",
       "          2.6084e+00,  5.4178e+00,  6.9552e+00,  1.7676e+00,  2.4191e+00,\n",
       "          4.2808e+00,  1.5591e+00, -8.5501e-01,  1.0503e+01,  3.2277e+00,\n",
       "          5.8277e+00,  1.2690e+01,  9.6679e-01,  1.5599e+00,  3.2586e+00,\n",
       "          7.3761e+00,  8.3188e+00,  2.5428e+00,  1.6168e+00,  7.2782e+00,\n",
       "          3.6493e+00,  1.8199e+00,  3.6996e+00,  7.3288e+00,  3.7992e+00,\n",
       "         -3.5228e+00,  8.7728e+00, -4.6105e-01,  1.1240e+00,  1.1329e+01,\n",
       "          4.2940e+00,  2.1682e+00,  2.3292e+00,  7.2227e+00,  1.1584e+01,\n",
       "          4.4288e+00,  2.8439e-01,  1.1631e+01, -1.0100e+00,  6.3701e+00,\n",
       "          2.0793e+00,  1.0616e+01, -4.0349e+00,  6.6493e+00,  6.5431e+00,\n",
       "          6.5514e+00, -4.1351e+00,  2.1088e+00, -3.1017e+00,  7.6686e+00,\n",
       "          3.8782e+00, -6.3129e+00,  5.4703e+00, -5.5494e-01,  3.6379e+00,\n",
       "          2.6842e+00,  8.7772e-01,  1.2930e+01,  2.8930e-01,  2.1837e+00,\n",
       "          8.1920e+00, -3.4228e+00,  1.1181e+01, -5.7166e-01,  5.6943e+00,\n",
       "          3.0997e+00,  5.1598e+00,  7.2185e+00,  4.1242e+00, -5.8402e+00,\n",
       "          3.3913e+00,  4.8131e+00,  1.0209e+01,  8.7982e-01,  6.4591e+00,\n",
       "          9.4002e+00, -2.6955e-01, -2.0611e+00,  3.2864e+00, -6.0570e+00,\n",
       "          3.7281e+00,  8.2502e-01,  3.8769e+00,  4.1335e+00,  8.2118e-01,\n",
       "          3.9679e+00, -4.1070e+00,  1.3188e+00,  5.1342e+00,  3.3868e+00,\n",
       "         -1.5306e+00,  8.0321e+00, -4.3413e+00,  6.3971e+00,  5.4471e+00,\n",
       "          3.7885e+00,  6.6149e+00,  5.0104e+00,  1.2283e+01,  2.4531e+00,\n",
       "          3.5130e+00,  3.5894e+00,  7.3113e-01,  4.1885e+00, -4.3290e-01,\n",
       "          7.5514e+00,  3.2023e+00,  1.3005e+01,  1.3799e+00,  7.2510e+00,\n",
       "          2.9957e+00,  5.2071e+00, -4.3158e+00, -6.7139e+00,  1.8850e+00,\n",
       "          2.3638e+00,  4.0877e+00,  3.4180e+00,  1.5610e+00,  7.7191e+00,\n",
       "          6.8388e+00,  8.8680e+00,  4.4029e+00,  8.6468e+00,  3.7176e+00,\n",
       "          1.1428e+01,  3.5812e+00,  7.1342e+00,  3.8601e+00,  2.6415e+00,\n",
       "          1.2898e+01,  4.5134e+00, -1.2727e-01,  2.6191e+00,  8.4875e+00,\n",
       "          2.8831e+00,  5.2671e+00,  3.5534e+00,  4.7361e+00,  4.8696e+00,\n",
       "         -8.1950e-01,  6.7068e+00,  4.3682e+00,  4.8143e+00, -2.3364e+00,\n",
       "          5.0297e-01, -7.4890e+00,  1.5751e+00,  4.5651e+00,  5.9814e+00,\n",
       "         -1.7884e-01, -7.3328e-01,  4.9589e+00,  2.0420e+00,  2.6963e+00,\n",
       "          4.9009e-01,  2.5291e+00,  1.3839e+00,  3.3957e+00, -1.4193e-01,\n",
       "          5.9933e+00,  5.5059e+00,  9.7880e+00,  6.4622e+00,  4.5985e+00,\n",
       "          7.3818e+00,  7.0841e+00, -8.2279e-03,  3.6408e+00,  1.0828e+00,\n",
       "          1.4832e+00, -3.4741e-02,  2.1784e+00,  3.7008e+00,  3.8469e+00,\n",
       "          1.0464e+01,  3.3219e+00,  8.9053e+00,  9.8197e+00,  1.9230e+00]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_inputs = 2 \n",
    "num_examples = 1000 \n",
    "true_w = [2, -3.4] \n",
    "true_b = 4.2\n",
    "features = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float) \n",
    "labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n",
    "labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)\n",
    "features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-courage",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "使用 `𝐮𝐬𝐞_𝐬𝐯𝐠_𝐩𝐚𝐥𝐲()` 和 `𝐬𝐞𝐭_𝐟𝐢𝐠𝐬𝐢𝐳𝐞()` 两个函数来可视化所生成的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reserved-modification",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 设置图的尺寸\n",
    "def set_figsize(figsize=(3.5, 2.5)):\n",
    "    plt.rcParams['figure.figsize'] = figsize \n",
    "    \n",
    "set_figsize()\n",
    "plt.scatter(features[:, 1].numpy(), labels.numpy(), 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-signal",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 读取数据\n",
    "在模型训练的时候，需要遍历数据集并不断读取小批量的数据样本。这里本实验定义一个函数 `𝐝𝐚𝐭𝐚_𝐢𝐭𝐞𝐫()` 它每次返回 𝒃𝒂𝒕𝒄𝒉_𝒔𝒊𝒛𝒆 (批量大小)个随机样本的特征和标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "infinite-moderator",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples)) \n",
    "    \n",
    "    # 样本的读取顺序是随机的 \n",
    "    random.shuffle(indices) \n",
    "    \n",
    "    # 最后一次可能不足一个batch\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = torch.tensor(indices[i: min(i + batch_size, num_examples)])\n",
    "        \n",
    "        # 按行选取\n",
    "        yield features.index_select(dim=0, index=j), labels.index_select(0, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-occasions",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 构建模型\n",
    "在构建模型之前，需要将权重和偏置初始化。本实验将权重初始化成均值为0、标准差为 0.01的正态随机数，偏置初始化为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "affiliated-drilling",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0122],\n",
       "         [ 0.0004]]),\n",
       " tensor([0.]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor(np.random.normal(0, 0.01, (num_inputs, 1)), dtype=torch.float32) \n",
    "b = torch.zeros(1, dtype=torch.float32)\n",
    "w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-release",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在后面的模型训练中，需要对这些参数求梯度来迭代参数的值，因此要设置requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "speaking-baker",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.requires_grad_(requires_grad=True)\n",
    "b.requires_grad_(requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-atlas",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "使用 𝐦𝐦() 函数做矩阵乘法，来实现线性回归的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "excess-insulin",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def linreg(X, w, b):\n",
    "    return torch.mm(X, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-horror",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 损失函数和优化算法\n",
    "本实验使用`平方损失`来定义线性回归的损失函数。在实现中，我们需要把真实值 𝑦 变形成预测值 𝑦_h𝑎𝑡 形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "moving-tucson",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):\n",
    "    return (y_hat.view(y.size()) - y) ** 2 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-milan",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "以下的 sgd 函数实现了小批量随机梯度下降算法。它通过不断迭代模型参数来优化损失函数。这里自动求梯度模块计算得来提速是一个批量样本的梯度和。我们将它除以批量大小来得到平均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adverse-crack",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size): \n",
    "    for param in params:\n",
    "        # 注意这里更改param时用的param.data\n",
    "        param.data -= lr * param.grad / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-samuel",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 模型训练\n",
    "在训练过程中，模型将会多次迭代更新参数。在每次迭代中，根据当前读取的小批量数据样本(特征 x 和标签 y)，通过调用反向函数backward 计算小批量随机梯度，并调用优化算法 𝐬𝐠𝐝 迭代模型参数 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "loaded-ownership",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 2.361809\n",
      "epoch 2, loss 0.323957\n",
      "epoch 3, loss 0.044822\n"
     ]
    }
   ],
   "source": [
    "lr = 0.03\n",
    "num_epochs = 3\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "batch_size = 32\n",
    "\n",
    "# 训练模型一共需要num_epochs个迭代周期\n",
    "for epoch in range(num_epochs): \n",
    "    \n",
    "    # 在每一个迭代周期中，会使用训练数据集中所有样本一次, x和y分别是小批量样本的特征和标签\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        \n",
    "        # 这是一个二维向量\n",
    "        y_hat = net(X, w, b) \n",
    "        \n",
    "        # l是有关小批量X和y的损失\n",
    "        l = loss(y_hat, y).sum()  \n",
    "        \n",
    "        # 小批量的损失对模型参数求梯度\n",
    "        l.backward() \n",
    "        \n",
    "        # 使用小批量随机梯度下降迭代模型参数 \n",
    "        sgd([w, b], lr, batch_size) \n",
    "        \n",
    "        # 梯度清零\n",
    "        w.grad.data.zero_()\n",
    "        b.grad.data.zero_()\n",
    "        \n",
    "    train_l = loss(net(features, w, b), labels)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "understanding-drove",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, -3.4],\n",
       " tensor([[ 1.9281],\n",
       "         [-3.2420]], requires_grad=True))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_w, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bound-anxiety",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.2, tensor([3.9622], requires_grad=True))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_b, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-fabric",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 利用torch.nn实现线性回归\n",
    "Torch.nn 模块<br>\n",
    "+ Pytorch为神经网络设计的模块化接口，该模块定义了大量的神经网络层。<br>\n",
    "+ 𝐧𝐧 利用 `𝐚𝐮𝐭𝐨𝐠𝐫𝐚𝐝` 来定义模型，其核心数据结构是 `𝐌𝐨𝐝𝐮𝐥𝐞`。<br>\n",
    "\n",
    "下表给出了部分 𝐧𝐧 中所包含模块(其它模块可查阅官方API):\n",
    "\n",
    "|模块|作用|\n",
    "|:---:|:----:|\n",
    "|torch.nn.Module()|Module是所有神经网络模块的基类|\n",
    "|torch.nn.Linear()|Linear用于对输入数据进行线性变换|\n",
    "|torch.nn.Sequential()|Sequential是一个顺序容器, 其中模块的添加顺序与在构造函数中传递模块时的顺序相同|\n",
    "|torch.nn.MSELoss|MSELoss用于衡量输入 x 和目标 y 中每个元素之间的均方误差的标准。|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-distinction",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 读取数据\n",
    "PyTorch提供了 𝐝𝐚𝐭𝐚 库来读取数据。由于data常用作变量名，这里将导入的 𝐝𝐚𝐭𝐚 模块用 𝐃𝐚𝐭𝐚 代替。 对前面的读取数据部分可以使用 𝐝𝐚𝐭𝐚 库来处理。在每一次迭代中，使用 𝐃𝐚𝐭𝐚 随机读取包含10个数据 样本的小批量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "impaired-fountain",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.03\n",
    "batch_size = 10\n",
    "\n",
    "# 将训练数据的特征和标签组合\n",
    "dataset = Data.TensorDataset(features, labels)\n",
    "\n",
    "# 把 dataset 放入 DataLoader\n",
    "data_iter = Data.DataLoader(\n",
    "    dataset=dataset,         # torch TensorDataset format\n",
    "    batch_size=batch_size,   # mini batch size\n",
    "    shuffle=True,            # 是否打乱数据 (训练集一般需要进行打乱) \n",
    "    num_workers=2,           # 多线程来读数据，注意在Windows下需要设置为0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-cliff",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 构建模型\n",
    "构建模型的过程中，最常见的方法就是继承 𝐧𝐧. 𝐌𝐨𝐝𝐮𝐥𝐞, 然后构建自己的网络。<br>\n",
    "一个 𝐧𝐧.𝐌𝐨𝐝𝐮𝐥𝐞 实例需要包含一些层以及返回输出的前向传播方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "intimate-delta",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearNet(\n",
       "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, n_feature):\n",
    "        super(LinearNet, self).__init__() \n",
    "        self.linear = nn.Linear(n_feature, 1)\n",
    "        \n",
    "    # forward 定义前向传播\n",
    "    def forward(self, x): \n",
    "        y = self.linear(x) \n",
    "        return y\n",
    "    \n",
    "net = LinearNet(num_inputs)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-repeat",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "除了继承 𝐧𝐧.𝐌𝐨𝐮𝐝𝐥𝐞 来构建线性回归模型，还可以利用 𝐧𝐧.𝐒𝐞𝐪𝐮𝐞𝐧𝐭𝐢𝐚𝐥 结合 𝐧𝐧.𝐋𝐢𝐧𝐞𝐚𝐫 来搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eastern-investigator",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#写法一\n",
    "net = nn.Sequential( \n",
    "    nn.Linear(num_inputs, 1) # 此处可以添加其它层\n",
    ")\n",
    "\n",
    "# 写法二\n",
    "# net = nn.Sequential()\n",
    "# net.add_module('linear', nn.Linear(num_inputs, 1)) \n",
    "# net.add_module.........\n",
    "\n",
    "# 写法三\n",
    "# from collections import OrderedDict\n",
    "# net = nn.Sequential(OrderedDict([\n",
    "# ('linear', nn.Linear(num_inputs, 1)) \n",
    "# ......]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-catering",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 模型参数初始化\n",
    "在使用定义的模型 net 之前，需要对模型中的一些参数进行初始化。PyTorch 在 init 模块 中提供了许多初始化参数的方法。我们可以调用𝐢𝐧𝐢𝐭.𝐧𝐨𝐫𝐦𝐚𝐥模块通过正态分布对线性回归中的权重和偏差进行初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eight-maker",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.normal_(net[0].weight, mean=0, std=0.01)\n",
    "\n",
    "# 也可以直接修改bias的data:net[0].bias.data.fill_(0)\n",
    "nn.init.constant_(net[0].bias, val=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-costs",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 损失函数和优化\n",
    "Pytorch 在 𝑡𝑜𝑟𝑐h.𝑛𝑛 中提供了各种损失函数，这些损失函数实现为 𝐧𝐧.𝐌𝐨𝐝𝐮𝐥𝐞 的子类，可以将这些损失函数作为一种特殊的层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "particular-upset",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.MSELoss()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-lender",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Pytorch在 𝐭𝐨𝐫𝐜𝐡.𝐨𝐩𝐭𝐢𝐦 模块中提供了诸如 𝑺𝑮𝑫、𝑨𝒅𝒂𝒎 和 𝑹𝑴𝑺𝑷𝒓𝒐𝒐𝒑 等优化算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "focal-wyoming",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 梯度下降的学习率指定为0.03\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.03)\n",
    "\n",
    "# 可以为不同的子网络设置不同学习率\n",
    "# optimizer =optim.SGD([\n",
    "#     # 如果不指定学习率，则用默认的最外层学习率 \n",
    "#     {'params': net.subnet1.parameters()}，\n",
    "#     {'params': net.subnet2.parameters(), 'lr': 0.01}\n",
    "# ], lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-jewelry",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 模型训练\n",
    "训练模型时，可以调用 𝐨𝐩𝐭𝐢𝐦 中的 𝐬𝐭𝐞𝐩() 函数来迭代模型参数。<br>\n",
    "按照小批量随机梯度下降的定义，在 𝐬𝐭𝐞𝐩() 函数中指定批量大小，从而对批量中的样本梯度求平均。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "piano-acquisition",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.000231\n",
      "epoch 2, loss: 0.000053\n",
      "epoch 3, loss: 0.000096\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter: \n",
    "        output = net(X)\n",
    "        l = loss(output, y.view(-1, 1))\n",
    "\n",
    "        # 梯度清零，等价于net.zero_grad() \n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        l.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    print('epoch %d, loss: %f' % (epoch, l.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-letter",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 模型预测及评价\n",
    "对于分类问题，给定任一样本特征，模型可以预测每个输出类别的概率。<br>\n",
    "通常，我们把`预测概率最大`的类别作为输出类别。<br>\n",
    "如果它与真实类别(标签)一致，说明这次预测是正确的。我们使用准确率(accuracy)来评价模型的表现。<br>\n",
    "它等于正确预测数量与总预测数量之比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "going-intro",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(y_hat: torch.Tensor, y: torch.Tensor):\n",
    "    return (y_hat.argmax(dim=1) == y).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "first-villa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "verified-carol",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(data_iter, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-tobacco",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}